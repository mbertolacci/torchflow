% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{train_conditional_flow}
\alias{train_conditional_flow}
\title{Train a conditional flow model}
\usage{
train_conditional_flow(
  model,
  generate,
  optimizer = torch::optim_adam,
  n_epochs = 128,
  batch_size = 32,
  after_epoch = NULL,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{model}{A conditional flow model inheriting from \code{\link[=nn_conditional_flow]{nn_conditional_flow()}}.}

\item{generate}{A function that generates a batch of target and conditioning
samples; see above for details. This will be passed the current epoch number
as an argument.}

\item{optimizer}{The optimizer to use, e.g. \code{\link[torch:optim_adam]{torch::optim_adam()}}.}

\item{n_epochs}{The number of epochs to train for.}

\item{batch_size}{The batch size.}

\item{after_epoch}{A function to call after each epoch.}

\item{verbose}{Whether to print progress.}

\item{...}{Additional arguments to pass to the optimizer.}
}
\description{
Method to train a conditional flow model. This is a basic training loop with
the following steps:
}
\details{
The training algorithm is as follows. For each epoch:
\enumerate{
\item Generate (using \code{generate}) a batch of target and conditioning samples.
\item Loop over the batches of the epoch, performing a gradient descent step
for each batch. The batches are processed in order from the generated
samples.
\item Call \code{after_epoch} (if provided) with the current epoch and the generated
samples. This can be used to print test loss or any other tasks.
}

The \code{generate} function (called with the current epoch number as an argument)
should return a list with the following elements:
\itemize{
\item \code{target}: An \code{\link[=array]{array()}}, \code{\link[=matrix]{matrix()}}, or \code{\link[torch:torch_tensor]{torch::torch_tensor()}} of target
samples.
\item \code{conditioning}: An optional \code{\link[=array]{array()}}, \code{\link[=matrix]{matrix()}}, or \code{\link[torch:torch_tensor]{torch::torch_tensor()}}
of conditioning samples.
If returning \code{torch_tensor()} objects, take care that they are on the same
device as the model.
}

The generated samples are the choice of the user. You could generate new
samples each epoch, or share the same samples across epochs (noting that
the model may overfit in this case). In that latter case, it would be good
to permute the order of the samples each epoch.

The training may be stopped early. The original model object is modified in
place.
}
\examples{
library(torch)
model <- nn_sequential_conditional_flow(
  nn_affine_coupling_block(input_size = 2),
  nn_permutation_flow(input_size = 2),
  nn_affine_coupling_block(input_size = 2)
)
generate <- function(epoch) {
  list(target = 2 + torch_randn(1024, 2))
}
# In practice, the number of epochs should be larger
train_conditional_flow(model, generate, n_epochs = 2)
}
