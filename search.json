[{"path":"https://mbertolacci.github.io/torchflow/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 torchflow authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"torchflow","text":"vignette still construction Give mathematical details normalizing flows","code":""},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"creating-and-sampling-from-a-flow","dir":"Articles","previous_headings":"","what":"Creating and sampling from a flow","title":"torchflow","text":"simple two parameter normalising flow can created follows: flow five layers, alternate affine coupling blocks permutation flows. flow just standard torch nn_module can used usual way: first dimension input acts batch dimension, flow can used generate multiple samples . code actually implements sampling distribution represented flow. can also done directly using generate_from_conditional_flow function:","code":"flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2),   nn_permutation_flow(2),   nn_affine_coupling_block(2),   nn_permutation_flow(2),   nn_affine_coupling_block(2) ) x <- torch_randn(5, 2) flow_model(x) #> torch_tensor #> -0.0751 -2.9941 #>  0.4235  1.1748 #>  0.5745 -0.9133 #>  0.4188 -1.1355 #> -1.4920 -0.0350 #> [ CPUFloatType{5,2} ][ grad_fn = <CatBackward0> ] generate_from_conditional_flow(flow_model, 5) #> torch_tensor #> -1.1276 -0.8314 #>  1.1878  0.5310 #> -1.3484  0.5181 #>  0.1518 -0.3504 #>  0.9255  1.1614 #> [ CPUFloatType{5,2} ][ grad_fn = <ViewBackward0> ]"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"conditional-flow","dir":"Articles","previous_headings":"","what":"Conditional flow","title":"torchflow","text":"conditional flow takes additional input, conditioning variable, can used condition samples additional information. flow therefore encodes conditional distribution. following code creates conditional flow architecture unconditional flow defined additional conditioning variable dimension 3: can sample flow given conditioning variable follows: can also batch conditioning variables:","code":"flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 3),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 3),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 3) ) conditioning <- torch_randn(3) generate_from_conditional_flow(flow_model, 5, conditioning) #> torch_tensor #> -0.9678  1.8107 #> -0.9940  3.1715 #> -0.7483 -0.3285 #>  1.7618  2.0934 #> -0.5364 -1.1544 #> [ CPUFloatType{5,2} ][ grad_fn = <ViewBackward0> ] conditioning <- torch_randn(8, 3) generate_from_conditional_flow(flow_model, 5, conditioning) #> torch_tensor #> (1,.,.) =  #>   0.3131 -1.0033 #>   0.5838 -1.6721 #>   0.4789  0.2951 #>  -1.2789  1.3614 #>  -1.8303  2.3836 #>  -0.2212  0.4140 #>  -1.9381 -0.2255 #>  -0.3735 -0.2018 #>  #> (2,.,.) =  #>   0.4731  0.9786 #>   0.8850  0.9669 #>   0.4630 -0.1808 #>   0.1731 -0.5650 #>  -0.0258 -1.4617 #>  -1.5752 -0.4747 #>  -1.3238  1.0095 #>   0.0731 -0.0153 #>  #> (3,.,.) =  #>   0.1013  0.3570 #>   1.0908  0.7165 #>   0.2166  0.2296 #>   0.2617  0.3669 #>   0.1739 -0.6990 #>   0.9444 -0.8114 #>  -1.6868  2.3998 #>  -0.3998 -0.3646 #>  #> ... [the output was truncated (use n=-1 to disable)] #> [ CPUFloatType{5,8,2} ][ grad_fn = <ViewBackward0> ]"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"training-an-unconditional-flow","dir":"Articles","previous_headings":"","what":"Training an unconditional flow","title":"torchflow","text":"flows randomly initialised, samples follow interesting distribution. can instead train flow follow given distribution using samples distribution. Let us train flow match following distribution σ∼N+(0,1)\\sigma \\sim N^+(0, 1), μ∼N(0,σ2)\\mu \\sim N(0, \\sigma^2), N+(0,1)N^+(0, 1) half normal distribution mean 0 standard deviation 1. can generate samples distribution follows. Note take logarithm σ\\sigma parameter ensure real support; flow targets transformed distribution. function can used train flow match distribution using train_conditional_flow function: looks though test loss converged. can sample trained flow follows compare samples test set:  looks like reasonable approximation target distribution. can also look marginal histograms:  look okay.","code":"generate_samples <- function(...) {   n_samples <- 1024   sigma <- torch_abs(torch_randn(n_samples))   mu <- torch_randn(n_samples) * sigma   list(target = torch_stack(list(mu, torch_log(sigma)), 2)) }  generate_samples() #> $target #> torch_tensor #> -3.5053e-01 -1.2908e+00 #>  7.7810e-01 -8.0676e-01 #>  7.2347e-01 -5.4963e-01 #> -7.4897e-01  2.5491e-01 #> -1.1292e-01 -1.2973e+00 #> -2.0186e-01  1.5641e-01 #>  1.5741e-01  3.5692e-03 #> -2.5365e-02 -2.6668e+00 #> -6.2581e-02 -2.0603e+00 #>  3.1327e+00  8.3547e-01 #>  3.1994e-01  4.1298e-01 #> -2.0972e-02 -3.6560e+00 #>  3.3826e-02 -1.0247e+00 #>  8.7040e-01  1.8037e-01 #>  3.5335e-01 -1.0651e-02 #>  1.6830e-01 -1.1280e+00 #>  5.6344e-01  1.2522e-01 #> -3.2234e-01 -4.0254e-01 #> -1.3047e+00 -1.8022e-01 #>  2.3853e-01 -3.3072e-01 #> -2.5794e-02 -1.4487e-01 #> -6.2495e-01  1.7006e-01 #>  1.7253e-01  2.6927e-01 #>  1.1450e+00  8.1056e-01 #>  1.4560e-01 -1.8477e+00 #> -1.9488e-01 -1.5008e+00 #>  2.9652e-01 -1.4044e+00 #> -4.0865e-01 -2.0125e-01 #> -4.7235e-01 -7.3673e-01 #> -2.0839e-01  3.5881e-02 #> ... [the output was truncated (use n=-1 to disable)] #> [ CPUFloatType{1024,2} ] # Make an unconditional flow flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2),   nn_permutation_flow(2),   nn_affine_coupling_block(2),   nn_permutation_flow(2),   nn_affine_coupling_block(2) )  # Generate a test set test_set <- generate_samples()  # Train the flow train_conditional_flow(   flow_model,   generate_samples,   n_epochs = 128,   batch_size = 1024,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(flow_model(test_set$target)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 1.019454  #> = Starting epoch 2  #> Test loss: 0.8001729  #> = Starting epoch 3  #> Test loss: 0.6349535  #> = Starting epoch 4  #> Test loss: 0.6364204  #> = Starting epoch 5  #> Test loss: 0.5503579  #> = Starting epoch 6  #> Test loss: 0.5780155  #> = Starting epoch 7  #> Test loss: 0.6128637  #> = Starting epoch 8  #> Test loss: 0.5061079  #> = Starting epoch 9  #> Test loss: 0.4522567  #> = Starting epoch 10  #> Test loss: 0.461656  #> = Starting epoch 11  #> Test loss: 0.4804883  #> = Starting epoch 12  #> Test loss: 0.4708935  #> = Starting epoch 13  #> Test loss: 0.4562798  #> = Starting epoch 14  #> Test loss: 0.4572525  #> = Starting epoch 15  #> Test loss: 0.4597514  #> = Starting epoch 16  #> Test loss: 0.4482262  #> = Starting epoch 17  #> Test loss: 0.4273974  #> = Starting epoch 18  #> Test loss: 0.416669  #> = Starting epoch 19  #> Test loss: 0.4165396  #> = Starting epoch 20  #> Test loss: 0.4151311  #> = Starting epoch 21  #> Test loss: 0.4097677  #> = Starting epoch 22  #> Test loss: 0.4077833  #> = Starting epoch 23  #> Test loss: 0.403617  #> = Starting epoch 24  #> Test loss: 0.3967237  #> = Starting epoch 25  #> Test loss: 0.3883187  #> = Starting epoch 26  #> Test loss: 0.3811233  #> = Starting epoch 27  #> Test loss: 0.3771731  #> = Starting epoch 28  #> Test loss: 0.3778552  #> = Starting epoch 29  #> Test loss: 0.374093  #> = Starting epoch 30  #> Test loss: 0.371003  #> = Starting epoch 31  #> Test loss: 0.3701437  #> = Starting epoch 32  #> Test loss: 0.3660832  #> = Starting epoch 33  #> Test loss: 0.3595401  #> = Starting epoch 34  #> Test loss: 0.3551197  #> = Starting epoch 35  #> Test loss: 0.3531817  #> = Starting epoch 36  #> Test loss: 0.352358  #> = Starting epoch 37  #> Test loss: 0.3485259  #> = Starting epoch 38  #> Test loss: 0.3469201  #> = Starting epoch 39  #> Test loss: 0.3451678  #> = Starting epoch 40  #> Test loss: 0.3437158  #> = Starting epoch 41  #> Test loss: 0.34465  #> = Starting epoch 42  #> Test loss: 0.3414521  #> = Starting epoch 43  #> Test loss: 0.3388229  #> = Starting epoch 44  #> Test loss: 0.3397154  #> = Starting epoch 45  #> Test loss: 0.3367679  #> = Starting epoch 46  #> Test loss: 0.340991  #> = Starting epoch 47  #> Test loss: 0.3466052  #> = Starting epoch 48  #> Test loss: 0.3398133  #> = Starting epoch 49  #> Test loss: 0.3391019  #> = Starting epoch 50  #> Test loss: 0.3399772  #> = Starting epoch 51  #> Test loss: 0.3383148  #> = Starting epoch 52  #> Test loss: 0.3369095  #> = Starting epoch 53  #> Test loss: 0.3338318  #> = Starting epoch 54  #> Test loss: 0.3322625  #> = Starting epoch 55  #> Test loss: 0.3354775  #> = Starting epoch 56  #> Test loss: 0.3335218  #> = Starting epoch 57  #> Test loss: 0.3363089  #> = Starting epoch 58  #> Test loss: 0.3321594  #> = Starting epoch 59  #> Test loss: 0.3269516  #> = Starting epoch 60  #> Test loss: 0.3387993  #> = Starting epoch 61  #> Test loss: 0.342851  #> = Starting epoch 62  #> Test loss: 0.3356961  #> = Starting epoch 63  #> Test loss: 0.3334178  #> = Starting epoch 64  #> Test loss: 0.3330007  #> = Starting epoch 65  #> Test loss: 0.3339243  #> = Starting epoch 66  #> Test loss: 0.3312158  #> = Starting epoch 67  #> Test loss: 0.3389346  #> = Starting epoch 68  #> Test loss: 0.3418733  #> = Starting epoch 69  #> Test loss: 0.3317673  #> = Starting epoch 70  #> Test loss: 0.3299628  #> = Starting epoch 71  #> Test loss: 0.337688  #> = Starting epoch 72  #> Test loss: 0.3268774  #> = Starting epoch 73  #> Test loss: 0.3326724  #> = Starting epoch 74  #> Test loss: 0.3402392  #> = Starting epoch 75  #> Test loss: 0.3356896  #> = Starting epoch 76  #> Test loss: 0.3317395  #> = Starting epoch 77  #> Test loss: 0.3319153  #> = Starting epoch 78  #> Test loss: 0.3298986  #> = Starting epoch 79  #> Test loss: 0.3322312  #> = Starting epoch 80  #> Test loss: 0.3381491  #> = Starting epoch 81  #> Test loss: 0.3405918  #> = Starting epoch 82  #> Test loss: 0.3309451  #> = Starting epoch 83  #> Test loss: 0.3412479  #> = Starting epoch 84  #> Test loss: 0.3667823  #> = Starting epoch 85  #> Test loss: 0.3664042  #> = Starting epoch 86  #> Test loss: 0.3268967  #> = Starting epoch 87  #> Test loss: 0.3565053  #> = Starting epoch 88  #> Test loss: 0.347115  #> = Starting epoch 89  #> Test loss: 0.3358175  #> = Starting epoch 90  #> Test loss: 0.3457328  #> = Starting epoch 91  #> Test loss: 0.3432286  #> = Starting epoch 92  #> Test loss: 0.3375158  #> = Starting epoch 93  #> Test loss: 0.3382624  #> = Starting epoch 94  #> Test loss: 0.3416291  #> = Starting epoch 95  #> Test loss: 0.346805  #> = Starting epoch 96  #> Test loss: 0.3416302  #> = Starting epoch 97  #> Test loss: 0.3387005  #> = Starting epoch 98  #> Test loss: 0.3392702  #> = Starting epoch 99  #> Test loss: 0.3384513  #> = Starting epoch 100  #> Test loss: 0.3355488  #> = Starting epoch 101  #> Test loss: 0.3446046  #> = Starting epoch 102  #> Test loss: 0.348417  #> = Starting epoch 103  #> Test loss: 0.3332508  #> = Starting epoch 104  #> Test loss: 0.3326703  #> = Starting epoch 105  #> Test loss: 0.3444866  #> = Starting epoch 106  #> Test loss: 0.3404641  #> = Starting epoch 107  #> Test loss: 0.3283109  #> = Starting epoch 108  #> Test loss: 0.3416814  #> = Starting epoch 109  #> Test loss: 0.3414527  #> = Starting epoch 110  #> Test loss: 0.3413328  #> = Starting epoch 111  #> Test loss: 0.330931  #> = Starting epoch 112  #> Test loss: 0.3288894  #> = Starting epoch 113  #> Test loss: 0.3284351  #> = Starting epoch 114  #> Test loss: 0.3331745  #> = Starting epoch 115  #> Test loss: 0.3356014  #> = Starting epoch 116  #> Test loss: 0.3350708  #> = Starting epoch 117  #> Test loss: 0.3295379  #> = Starting epoch 118  #> Test loss: 0.3365108  #> = Starting epoch 119  #> Test loss: 0.3298629  #> = Starting epoch 120  #> Test loss: 0.3318798  #> = Starting epoch 121  #> Test loss: 0.3276694  #> = Starting epoch 122  #> Test loss: 0.3304611  #> = Starting epoch 123  #> Test loss: 0.3243593  #> = Starting epoch 124  #> Test loss: 0.3233746  #> = Starting epoch 125  #> Test loss: 0.3234521  #> = Starting epoch 126  #> Test loss: 0.333637  #> = Starting epoch 127  #> Test loss: 0.3235033  #> = Starting epoch 128  #> Test loss: 0.3294823 test_samples <- as_array(generate_from_conditional_flow(flow_model, 1024)) test_target <- as_array(test_set$target) plot(test_target, xlab = 'mu', ylab = 'log(sigma)') points(test_samples, col = 'red') par(mfrow = c(2, 2)) hist(test_target[, 1], main = 'Target', xlab = 'mu', freq = FALSE, breaks = 32) hist(test_samples[, 1], main = 'Samples', xlab = 'mu', freq = FALSE, breaks = 32) hist(test_target[, 2], main = 'Target', xlab = 'log(sigma)', freq = FALSE, breaks = 32) hist(test_samples[, 2], main = 'Samples', xlab = 'log(sigma)', freq = FALSE, breaks = 32)"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"training-a-conditional-flow","dir":"Articles","previous_headings":"","what":"Training a conditional flow","title":"torchflow","text":"process training conditional flow unconditional flow, except generate function now also returns conditioning variable. Let’s add conditioning variable, y∼N(μ,σ2)y \\sim N(\\mu, \\sigma^2), four replicates: can generate samples trained flow follows, now samples conditioned values yy:  can also plot samples scatter plot:  Compare MCMC","code":"generate_conditional_samples <- function(...) {   n_samples <- 1024   sigma <- torch_abs(torch_randn(n_samples))   mu <- torch_randn(n_samples) * sigma   y <- torch_unsqueeze(mu, 2) + torch_randn(n_samples, 4) * torch_unsqueeze(sigma, 2)   list(     target = torch_stack(list(mu, torch_log(sigma)), 2),     conditioning = y   ) }  # Make a conditional flow flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 4),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 4),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 4) )  # Generate a test set test_set <- generate_conditional_samples() str(test_set) #> List of 2 #>  $ target      :Float [1:1024, 1:2] #>  $ conditioning:Float [1:1024, 1:4]  # Train the flow train_conditional_flow(   flow_model,   generate_conditional_samples,   n_epochs = 256,   batch_size = 1024,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(flow_model(test_set$target, test_set$conditioning)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 0.974283  #> = Starting epoch 2  #> Test loss: 0.6859506  #> = Starting epoch 3  #> Test loss: 0.4407223  #> = Starting epoch 4  #> Test loss: 0.2286544  #> = Starting epoch 5  #> Test loss: 0.06232631  #> = Starting epoch 6  #> Test loss: -0.05996329  #> = Starting epoch 7  #> Test loss: -0.1110364  #> = Starting epoch 8  #> Test loss: -0.2104945  #> = Starting epoch 9  #> Test loss: -0.2747762  #> = Starting epoch 10  #> Test loss: -0.3281401  #> = Starting epoch 11  #> Test loss: -0.4086823  #> = Starting epoch 12  #> Test loss: -0.4804138  #> = Starting epoch 13  #> Test loss: -0.5024436  #> = Starting epoch 14  #> Test loss: -0.514145  #> = Starting epoch 15  #> Test loss: -0.574394  #> = Starting epoch 16  #> Test loss: -0.6340774  #> = Starting epoch 17  #> Test loss: -0.6833488  #> = Starting epoch 18  #> Test loss: -0.6969036  #> = Starting epoch 19  #> Test loss: -0.7336826  #> = Starting epoch 20  #> Test loss: -0.7631932  #> = Starting epoch 21  #> Test loss: -0.7932017  #> = Starting epoch 22  #> Test loss: -0.8157225  #> = Starting epoch 23  #> Test loss: -0.8393176  #> = Starting epoch 24  #> Test loss: -0.8545727  #> = Starting epoch 25  #> Test loss: -0.8739516  #> = Starting epoch 26  #> Test loss: -0.9000497  #> = Starting epoch 27  #> Test loss: -0.9303639  #> = Starting epoch 28  #> Test loss: -0.9479313  #> = Starting epoch 29  #> Test loss: -0.979469  #> = Starting epoch 30  #> Test loss: -1.015208  #> = Starting epoch 31  #> Test loss: -1.025735  #> = Starting epoch 32  #> Test loss: -1.021235  #> = Starting epoch 33  #> Test loss: -1.031621  #> = Starting epoch 34  #> Test loss: -1.042001  #> = Starting epoch 35  #> Test loss: -1.05303  #> = Starting epoch 36  #> Test loss: -1.086175  #> = Starting epoch 37  #> Test loss: -1.108765  #> = Starting epoch 38  #> Test loss: -1.115904  #> = Starting epoch 39  #> Test loss: -1.116636  #> = Starting epoch 40  #> Test loss: -1.112639  #> = Starting epoch 41  #> Test loss: -1.112478  #> = Starting epoch 42  #> Test loss: -1.123207  #> = Starting epoch 43  #> Test loss: -1.139176  #> = Starting epoch 44  #> Test loss: -1.144642  #> = Starting epoch 45  #> Test loss: -1.143717  #> = Starting epoch 46  #> Test loss: -1.143407  #> = Starting epoch 47  #> Test loss: -1.142956  #> = Starting epoch 48  #> Test loss: -1.149074  #> = Starting epoch 49  #> Test loss: -1.157536  #> = Starting epoch 50  #> Test loss: -1.167896  #> = Starting epoch 51  #> Test loss: -1.1751  #> = Starting epoch 52  #> Test loss: -1.1733  #> = Starting epoch 53  #> Test loss: -1.168831  #> = Starting epoch 54  #> Test loss: -1.167374  #> = Starting epoch 55  #> Test loss: -1.172592  #> = Starting epoch 56  #> Test loss: -1.180207  #> = Starting epoch 57  #> Test loss: -1.19069  #> = Starting epoch 58  #> Test loss: -1.200391  #> = Starting epoch 59  #> Test loss: -1.201184  #> = Starting epoch 60  #> Test loss: -1.207329  #> = Starting epoch 61  #> Test loss: -1.215403  #> = Starting epoch 62  #> Test loss: -1.222946  #> = Starting epoch 63  #> Test loss: -1.215244  #> = Starting epoch 64  #> Test loss: -1.198149  #> = Starting epoch 65  #> Test loss: -1.215853  #> = Starting epoch 66  #> Test loss: -1.224196  #> = Starting epoch 67  #> Test loss: -1.220512  #> = Starting epoch 68  #> Test loss: -1.238345  #> = Starting epoch 69  #> Test loss: -1.243337  #> = Starting epoch 70  #> Test loss: -1.239646  #> = Starting epoch 71  #> Test loss: -1.219858  #> = Starting epoch 72  #> Test loss: -1.230498  #> = Starting epoch 73  #> Test loss: -1.252327  #> = Starting epoch 74  #> Test loss: -1.254562  #> = Starting epoch 75  #> Test loss: -1.24921  #> = Starting epoch 76  #> Test loss: -1.240478  #> = Starting epoch 77  #> Test loss: -1.256907  #> = Starting epoch 78  #> Test loss: -1.253419  #> = Starting epoch 79  #> Test loss: -1.239894  #> = Starting epoch 80  #> Test loss: -1.230754  #> = Starting epoch 81  #> Test loss: -1.239485  #> = Starting epoch 82  #> Test loss: -1.267896  #> = Starting epoch 83  #> Test loss: -1.253018  #> = Starting epoch 84  #> Test loss: -1.264933  #> = Starting epoch 85  #> Test loss: -1.270409  #> = Starting epoch 86  #> Test loss: -1.264064  #> = Starting epoch 87  #> Test loss: -1.274047  #> = Starting epoch 88  #> Test loss: -1.267729  #> = Starting epoch 89  #> Test loss: -1.284427  #> = Starting epoch 90  #> Test loss: -1.27361  #> = Starting epoch 91  #> Test loss: -1.288056  #> = Starting epoch 92  #> Test loss: -1.28537  #> = Starting epoch 93  #> Test loss: -1.287618  #> = Starting epoch 94  #> Test loss: -1.296423  #> = Starting epoch 95  #> Test loss: -1.289456  #> = Starting epoch 96  #> Test loss: -1.294518  #> = Starting epoch 97  #> Test loss: -1.291477  #> = Starting epoch 98  #> Test loss: -1.296226  #> = Starting epoch 99  #> Test loss: -1.297531  #> = Starting epoch 100  #> Test loss: -1.303172  #> = Starting epoch 101  #> Test loss: -1.300063  #> = Starting epoch 102  #> Test loss: -1.296252  #> = Starting epoch 103  #> Test loss: -1.29763  #> = Starting epoch 104  #> Test loss: -1.304412  #> = Starting epoch 105  #> Test loss: -1.312131  #> = Starting epoch 106  #> Test loss: -1.311492  #> = Starting epoch 107  #> Test loss: -1.311706  #> = Starting epoch 108  #> Test loss: -1.315776  #> = Starting epoch 109  #> Test loss: -1.313412  #> = Starting epoch 110  #> Test loss: -1.31584  #> = Starting epoch 111  #> Test loss: -1.322912  #> = Starting epoch 112  #> Test loss: -1.327356  #> = Starting epoch 113  #> Test loss: -1.32618  #> = Starting epoch 114  #> Test loss: -1.32318  #> = Starting epoch 115  #> Test loss: -1.325952  #> = Starting epoch 116  #> Test loss: -1.327516  #> = Starting epoch 117  #> Test loss: -1.321315  #> = Starting epoch 118  #> Test loss: -1.322015  #> = Starting epoch 119  #> Test loss: -1.323695  #> = Starting epoch 120  #> Test loss: -1.326124  #> = Starting epoch 121  #> Test loss: -1.329335  #> = Starting epoch 122  #> Test loss: -1.327054  #> = Starting epoch 123  #> Test loss: -1.326412  #> = Starting epoch 124  #> Test loss: -1.330221  #> = Starting epoch 125  #> Test loss: -1.330231  #> = Starting epoch 126  #> Test loss: -1.331922  #> = Starting epoch 127  #> Test loss: -1.334515  #> = Starting epoch 128  #> Test loss: -1.336287  #> = Starting epoch 129  #> Test loss: -1.332808  #> = Starting epoch 130  #> Test loss: -1.332172  #> = Starting epoch 131  #> Test loss: -1.329735  #> = Starting epoch 132  #> Test loss: -1.323523  #> = Starting epoch 133  #> Test loss: -1.323841  #> = Starting epoch 134  #> Test loss: -1.329269  #> = Starting epoch 135  #> Test loss: -1.333993  #> = Starting epoch 136  #> Test loss: -1.332384  #> = Starting epoch 137  #> Test loss: -1.33024  #> = Starting epoch 138  #> Test loss: -1.328507  #> = Starting epoch 139  #> Test loss: -1.332928  #> = Starting epoch 140  #> Test loss: -1.341386  #> = Starting epoch 141  #> Test loss: -1.331816  #> = Starting epoch 142  #> Test loss: -1.341086  #> = Starting epoch 143  #> Test loss: -1.345632  #> = Starting epoch 144  #> Test loss: -1.34897  #> = Starting epoch 145  #> Test loss: -1.341655  #> = Starting epoch 146  #> Test loss: -1.344538  #> = Starting epoch 147  #> Test loss: -1.347535  #> = Starting epoch 148  #> Test loss: -1.348025  #> = Starting epoch 149  #> Test loss: -1.352947  #> = Starting epoch 150  #> Test loss: -1.347746  #> = Starting epoch 151  #> Test loss: -1.35386  #> = Starting epoch 152  #> Test loss: -1.340459  #> = Starting epoch 153  #> Test loss: -1.352573  #> = Starting epoch 154  #> Test loss: -1.349212  #> = Starting epoch 155  #> Test loss: -1.34439  #> = Starting epoch 156  #> Test loss: -1.340122  #> = Starting epoch 157  #> Test loss: -1.344616  #> = Starting epoch 158  #> Test loss: -1.346093  #> = Starting epoch 159  #> Test loss: -1.356203  #> = Starting epoch 160  #> Test loss: -1.361074  #> = Starting epoch 161  #> Test loss: -1.363306  #> = Starting epoch 162  #> Test loss: -1.360559  #> = Starting epoch 163  #> Test loss: -1.358203  #> = Starting epoch 164  #> Test loss: -1.357812  #> = Starting epoch 165  #> Test loss: -1.368118  #> = Starting epoch 166  #> Test loss: -1.371539  #> = Starting epoch 167  #> Test loss: -1.368302  #> = Starting epoch 168  #> Test loss: -1.365618  #> = Starting epoch 169  #> Test loss: -1.364403  #> = Starting epoch 170  #> Test loss: -1.352534  #> = Starting epoch 171  #> Test loss: -1.363319  #> = Starting epoch 172  #> Test loss: -1.360164  #> = Starting epoch 173  #> Test loss: -1.372436  #> = Starting epoch 174  #> Test loss: -1.328156  #> = Starting epoch 175  #> Test loss: -1.3676  #> = Starting epoch 176  #> Test loss: -1.369272  #> = Starting epoch 177  #> Test loss: -1.380396  #> = Starting epoch 178  #> Test loss: -1.374346  #> = Starting epoch 179  #> Test loss: -1.374972  #> = Starting epoch 180  #> Test loss: -1.351846  #> = Starting epoch 181  #> Test loss: -1.382649  #> = Starting epoch 182  #> Test loss: -1.356667  #> = Starting epoch 183  #> Test loss: -1.340237  #> = Starting epoch 184  #> Test loss: -1.375568  #> = Starting epoch 185  #> Test loss: -1.367621  #> = Starting epoch 186  #> Test loss: -1.370498  #> = Starting epoch 187  #> Test loss: -1.350887  #> = Starting epoch 188  #> Test loss: -1.367533  #> = Starting epoch 189  #> Test loss: -1.385736  #> = Starting epoch 190  #> Test loss: -1.369837  #> = Starting epoch 191  #> Test loss: -1.383772  #> = Starting epoch 192  #> Test loss: -1.373187  #> = Starting epoch 193  #> Test loss: -1.352001  #> = Starting epoch 194  #> Test loss: -1.279801  #> = Starting epoch 195  #> Test loss: -1.267049  #> = Starting epoch 196  #> Test loss: -1.334275  #> = Starting epoch 197  #> Test loss: -1.354821  #> = Starting epoch 198  #> Test loss: -1.258368  #> = Starting epoch 199  #> Test loss: -1.361836  #> = Starting epoch 200  #> Test loss: -1.330196  #> = Starting epoch 201  #> Test loss: -1.292997  #> = Starting epoch 202  #> Test loss: -1.314085  #> = Starting epoch 203  #> Test loss: -1.361861  #> = Starting epoch 204  #> Test loss: -1.344581  #> = Starting epoch 205  #> Test loss: -1.317917  #> = Starting epoch 206  #> Test loss: -1.357548  #> = Starting epoch 207  #> Test loss: -1.374592  #> = Starting epoch 208  #> Test loss: -1.362281  #> = Starting epoch 209  #> Test loss: -1.36156  #> = Starting epoch 210  #> Test loss: -1.372641  #> = Starting epoch 211  #> Test loss: -1.367548  #> = Starting epoch 212  #> Test loss: -1.33332  #> = Starting epoch 213  #> Test loss: -1.342221  #> = Starting epoch 214  #> Test loss: -1.362857  #> = Starting epoch 215  #> Test loss: -1.360852  #> = Starting epoch 216  #> Test loss: -1.356888  #> = Starting epoch 217  #> Test loss: -1.355878  #> = Starting epoch 218  #> Test loss: -1.349068  #> = Starting epoch 219  #> Test loss: -1.358039  #> = Starting epoch 220  #> Test loss: -1.369555  #> = Starting epoch 221  #> Test loss: -1.363123  #> = Starting epoch 222  #> Test loss: -1.353819  #> = Starting epoch 223  #> Test loss: -1.358652  #> = Starting epoch 224  #> Test loss: -1.364319  #> = Starting epoch 225  #> Test loss: -1.360475  #> = Starting epoch 226  #> Test loss: -1.371809  #> = Starting epoch 227  #> Test loss: -1.375602  #> = Starting epoch 228  #> Test loss: -1.375688  #> = Starting epoch 229  #> Test loss: -1.381152  #> = Starting epoch 230  #> Test loss: -1.386556  #> = Starting epoch 231  #> Test loss: -1.38534  #> = Starting epoch 232  #> Test loss: -1.383651  #> = Starting epoch 233  #> Test loss: -1.38791  #> = Starting epoch 234  #> Test loss: -1.388513  #> = Starting epoch 235  #> Test loss: -1.383379  #> = Starting epoch 236  #> Test loss: -1.382724  #> = Starting epoch 237  #> Test loss: -1.370929  #> = Starting epoch 238  #> Test loss: -1.352974  #> = Starting epoch 239  #> Test loss: -1.360017  #> = Starting epoch 240  #> Test loss: -1.370719  #> = Starting epoch 241  #> Test loss: -1.38357  #> = Starting epoch 242  #> Test loss: -1.390603  #> = Starting epoch 243  #> Test loss: -1.386653  #> = Starting epoch 244  #> Test loss: -1.38572  #> = Starting epoch 245  #> Test loss: -1.388134  #> = Starting epoch 246  #> Test loss: -1.39222  #> = Starting epoch 247  #> Test loss: -1.391302  #> = Starting epoch 248  #> Test loss: -1.389304  #> = Starting epoch 249  #> Test loss: -1.389257  #> = Starting epoch 250  #> Test loss: -1.392479  #> = Starting epoch 251  #> Test loss: -1.396519  #> = Starting epoch 252  #> Test loss: -1.401493  #> = Starting epoch 253  #> Test loss: -1.398555  #> = Starting epoch 254  #> Test loss: -1.391079  #> = Starting epoch 255  #> Test loss: -1.390803  #> = Starting epoch 256  #> Test loss: -1.393021 # Generate 1024 samples for each of the first 4 conditioning variables in the test set test_samples <- as_array(generate_from_conditional_flow(flow_model, 1024, test_set$conditioning[1 : 4, ])) test_target <- as_array(test_set$target)  par(mfrow = c(4, 2)) for (i in 1 : 4) {   hist(test_samples[, i, 1], main = '', xlab = 'mu', freq = FALSE, breaks = 32, xlim = c(-3, 3))   abline(v = as_array(test_set$conditioning[i, ]), col = 'blue')   abline(v = test_target[i, 1], col = 'red')   hist(test_samples[, i, 2], main = '', xlab = 'log(sigma)', freq = FALSE, breaks = 32, xlim = c(-8, 8))   abline(v = test_target[i, 2], col = 'red') } par(mfrow = c(2, 2)) for (i in 1 : 4) {   plot(     test_samples[, i, 1], test_samples[, i, 2], main = '',     xlab = 'mu', ylab = 'log(sigma)', xlim = c(-3, 3), ylim = c(-8, 8)   )   abline(v = test_target[i, 1], col = 'red')   abline(h = test_target[i, 2], col = 'red') }"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"using-a-summarizing-network","dir":"Articles","previous_headings":"","what":"Using a summarizing network","title":"torchflow","text":"example, conditioning variable y contains four replicates conditioning variable. ignored fact replicates, trained flow though dependent. can instead use summarizing network individually processes individual replicate set summary statistics, combine summary statistics permutation invariant way form conditioning variable. example summarizing network: can combine summarizing network flow nn_summarizing_conditional_flow object: Let’s also expand number replicated observations 32: Let’s train model: can generate samples trained model :  can also plot samples scatter plot:","code":"# Helper modules nn_sum <- nn_module(   initialize = function(dimension) {     self$dimension <- dimension   },   forward = function(x) {     torch_sum(x, self$dimension)   } )  nn_unsqueeze <- nn_module(   initialize = function(dimension) {     self$dimension <- dimension   },   forward = function(x) {     torch_unsqueeze(x, self$dimension)   } )  summary_model <- nn_sequential(   # Add an extra unit dimension for the replicate dimension   nn_unsqueeze(-1),   # Compute the summary statistics for each replicate   nn_linear(1, 32),   nn_relu(),   nn_linear(32, 32),   nn_relu(),   nn_linear(32, 8),   # Sum the summary statistics across the replicates   nn_sum(-2) )  summary_model(test_set$conditioning[1 : 10, , drop = FALSE]) #> torch_tensor #> -0.3106 -0.4794 -1.0355 -0.1156  0.7861 -0.1289  0.4472  0.6489 #> -0.3270 -0.4876 -1.0002 -0.1192  0.8086 -0.1651  0.4307  0.6599 #>  0.8136  0.7213 -2.3030 -0.7856  1.1316 -2.2367 -0.0026  4.6636 #> -0.1867 -0.6734 -1.5861  0.1312  0.7808  0.5128  0.4095  0.4956 #> -0.3742 -0.5113 -1.1086 -0.0737  0.7192  0.0581  0.4735  0.4543 #> -0.3475 -0.5235 -1.1685 -0.0544  0.7058  0.1280  0.4847  0.4540 #> -0.3841 -0.5066 -1.0868 -0.0812  0.7250  0.0332  0.4692  0.4545 #>  0.1397 -0.0340 -1.3028 -0.3851  0.9523 -0.9552  0.1906  2.1036 #> -0.3189 -0.5128 -1.0572 -0.0948  0.7766 -0.0629  0.4657  0.5951 #> -0.3926 -0.5024 -1.0674 -0.0883  0.7327  0.0119  0.4651  0.4556 #> [ CPUFloatType{10,8} ][ grad_fn = <SumBackward1> ] flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 8),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 8),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 8) ) summarizing_flow_model <- nn_summarizing_conditional_flow(summary_model, flow_model) generate_conditional_samples <- function(...) {   n_samples <- 1024   sigma <- torch_abs(torch_randn(n_samples))   mu <- torch_randn(n_samples) * sigma   y <- torch_unsqueeze(mu, 2) + torch_randn(n_samples, 32) * torch_unsqueeze(sigma, 2)   list(     target = torch_stack(list(mu, torch_log(sigma)), 2),     conditioning = y   ) } test_set <- generate_conditional_samples() str(test_set) #> List of 2 #>  $ target      :Float [1:1024, 1:2] #>  $ conditioning:Float [1:1024, 1:32] train_conditional_flow(   summarizing_flow_model,   generate_conditional_samples,   n_epochs = 256,   batch_size = 1024,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(summarizing_flow_model(test_set$target, test_set$conditioning)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 1.526718  #> = Starting epoch 2  #> Test loss: 1.536858  #> = Starting epoch 3  #> Test loss: 1.450058  #> = Starting epoch 4  #> Test loss: 1.743826  #> = Starting epoch 5  #> Test loss: 1.219582  #> = Starting epoch 6  #> Test loss: 0.9647278  #> = Starting epoch 7  #> Test loss: 0.7783312  #> = Starting epoch 8  #> Test loss: 0.5277256  #> = Starting epoch 9  #> Test loss: 0.2146801  #> = Starting epoch 10  #> Test loss: -0.1401669  #> = Starting epoch 11  #> Test loss: -0.4295816  #> = Starting epoch 12  #> Test loss: -0.604944  #> = Starting epoch 13  #> Test loss: -0.7457875  #> = Starting epoch 14  #> Test loss: -0.9327806  #> = Starting epoch 15  #> Test loss: -1.124381  #> = Starting epoch 16  #> Test loss: -1.305997  #> = Starting epoch 17  #> Test loss: -1.335194  #> = Starting epoch 18  #> Test loss: -1.511848  #> = Starting epoch 19  #> Test loss: -1.722389  #> = Starting epoch 20  #> Test loss: -1.852758  #> = Starting epoch 21  #> Test loss: -1.778213  #> = Starting epoch 22  #> Test loss: -1.91583  #> = Starting epoch 23  #> Test loss: -2.013683  #> = Starting epoch 24  #> Test loss: -2.026929  #> = Starting epoch 25  #> Test loss: -2.118891  #> = Starting epoch 26  #> Test loss: -2.198356  #> = Starting epoch 27  #> Test loss: -2.105805  #> = Starting epoch 28  #> Test loss: -2.101241  #> = Starting epoch 29  #> Test loss: -2.364888  #> = Starting epoch 30  #> Test loss: -2.313214  #> = Starting epoch 31  #> Test loss: -2.302035  #> = Starting epoch 32  #> Test loss: -2.313225  #> = Starting epoch 33  #> Test loss: -2.506254  #> = Starting epoch 34  #> Test loss: -2.402857  #> = Starting epoch 35  #> Test loss: -2.514345  #> = Starting epoch 36  #> Test loss: -2.494533  #> = Starting epoch 37  #> Test loss: -2.520484  #> = Starting epoch 38  #> Test loss: -2.484  #> = Starting epoch 39  #> Test loss: -2.576917  #> = Starting epoch 40  #> Test loss: -2.677201  #> = Starting epoch 41  #> Test loss: -2.665617  #> = Starting epoch 42  #> Test loss: -2.69583  #> = Starting epoch 43  #> Test loss: -2.672577  #> = Starting epoch 44  #> Test loss: -2.707613  #> = Starting epoch 45  #> Test loss: -2.713792  #> = Starting epoch 46  #> Test loss: -2.742512  #> = Starting epoch 47  #> Test loss: -2.795876  #> = Starting epoch 48  #> Test loss: -2.769366  #> = Starting epoch 49  #> Test loss: -2.813559  #> = Starting epoch 50  #> Test loss: -2.787848  #> = Starting epoch 51  #> Test loss: -2.843822  #> = Starting epoch 52  #> Test loss: -2.830829  #> = Starting epoch 53  #> Test loss: -2.845436  #> = Starting epoch 54  #> Test loss: -2.735476  #> = Starting epoch 55  #> Test loss: -2.537999  #> = Starting epoch 56  #> Test loss: -1.93173  #> = Starting epoch 57  #> Test loss: -2.888646  #> = Starting epoch 58  #> Test loss: -1.89783  #> = Starting epoch 59  #> Test loss: -2.125655  #> = Starting epoch 60  #> Test loss: -2.045037  #> = Starting epoch 61  #> Test loss: -2.844924  #> = Starting epoch 62  #> Test loss: -1.932356  #> = Starting epoch 63  #> Test loss: -2.804255  #> = Starting epoch 64  #> Test loss: -2.45432  #> = Starting epoch 65  #> Test loss: -2.323543  #> = Starting epoch 66  #> Test loss: -2.716306  #> = Starting epoch 67  #> Test loss: -2.698018  #> = Starting epoch 68  #> Test loss: -2.448188  #> = Starting epoch 69  #> Test loss: -2.792795  #> = Starting epoch 70  #> Test loss: -2.699057  #> = Starting epoch 71  #> Test loss: -2.585504  #> = Starting epoch 72  #> Test loss: -2.726729  #> = Starting epoch 73  #> Test loss: -2.823969  #> = Starting epoch 74  #> Test loss: -2.673913  #> = Starting epoch 75  #> Test loss: -2.790593  #> = Starting epoch 76  #> Test loss: -2.843045  #> = Starting epoch 77  #> Test loss: -2.738256  #> = Starting epoch 78  #> Test loss: -2.823659  #> = Starting epoch 79  #> Test loss: -2.891817  #> = Starting epoch 80  #> Test loss: -2.825991  #> = Starting epoch 81  #> Test loss: -2.876102  #> = Starting epoch 82  #> Test loss: -2.915362  #> = Starting epoch 83  #> Test loss: -2.861319  #> = Starting epoch 84  #> Test loss: -2.915531  #> = Starting epoch 85  #> Test loss: -2.936903  #> = Starting epoch 86  #> Test loss: -2.879354  #> = Starting epoch 87  #> Test loss: -2.941741  #> = Starting epoch 88  #> Test loss: -2.931585  #> = Starting epoch 89  #> Test loss: -2.923904  #> = Starting epoch 90  #> Test loss: -2.975952  #> = Starting epoch 91  #> Test loss: -2.926932  #> = Starting epoch 92  #> Test loss: -2.986612  #> = Starting epoch 93  #> Test loss: -2.920679  #> = Starting epoch 94  #> Test loss: -2.960256  #> = Starting epoch 95  #> Test loss: -2.992249  #> = Starting epoch 96  #> Test loss: -2.960296  #> = Starting epoch 97  #> Test loss: -3.017703  #> = Starting epoch 98  #> Test loss: -2.970178  #> = Starting epoch 99  #> Test loss: -3.019058  #> = Starting epoch 100  #> Test loss: -2.951919  #> = Starting epoch 101  #> Test loss: -3.013816  #> = Starting epoch 102  #> Test loss: -2.998203  #> = Starting epoch 103  #> Test loss: -3.009127  #> = Starting epoch 104  #> Test loss: -3.002527  #> = Starting epoch 105  #> Test loss: -3.018548  #> = Starting epoch 106  #> Test loss: -3.033028  #> = Starting epoch 107  #> Test loss: -2.993316  #> = Starting epoch 108  #> Test loss: -2.995909  #> = Starting epoch 109  #> Test loss: -3.005055  #> = Starting epoch 110  #> Test loss: -3.010753  #> = Starting epoch 111  #> Test loss: -3.028734  #> = Starting epoch 112  #> Test loss: -2.971267  #> = Starting epoch 113  #> Test loss: -3.057925  #> = Starting epoch 114  #> Test loss: -2.987813  #> = Starting epoch 115  #> Test loss: -3.05714  #> = Starting epoch 116  #> Test loss: -2.941054  #> = Starting epoch 117  #> Test loss: -3.063341  #> = Starting epoch 118  #> Test loss: -2.990528  #> = Starting epoch 119  #> Test loss: -3.050192  #> = Starting epoch 120  #> Test loss: -2.98166  #> = Starting epoch 121  #> Test loss: -3.069734  #> = Starting epoch 122  #> Test loss: -3.028991  #> = Starting epoch 123  #> Test loss: -3.08167  #> = Starting epoch 124  #> Test loss: -3.008679  #> = Starting epoch 125  #> Test loss: -3.043858  #> = Starting epoch 126  #> Test loss: -3.038477  #> = Starting epoch 127  #> Test loss: -3.07714  #> = Starting epoch 128  #> Test loss: -3.049425  #> = Starting epoch 129  #> Test loss: -3.063778  #> = Starting epoch 130  #> Test loss: -3.096062  #> = Starting epoch 131  #> Test loss: -3.04287  #> = Starting epoch 132  #> Test loss: -3.066877  #> = Starting epoch 133  #> Test loss: -3.078909  #> = Starting epoch 134  #> Test loss: -3.076254  #> = Starting epoch 135  #> Test loss: -3.096772  #> = Starting epoch 136  #> Test loss: -3.099791  #> = Starting epoch 137  #> Test loss: -3.076819  #> = Starting epoch 138  #> Test loss: -3.128488  #> = Starting epoch 139  #> Test loss: -3.071347  #> = Starting epoch 140  #> Test loss: -3.126267  #> = Starting epoch 141  #> Test loss: -3.04581  #> = Starting epoch 142  #> Test loss: -3.138049  #> = Starting epoch 143  #> Test loss: -3.033208  #> = Starting epoch 144  #> Test loss: -3.003991  #> = Starting epoch 145  #> Test loss: -3.019236  #> = Starting epoch 146  #> Test loss: -3.062065  #> = Starting epoch 147  #> Test loss: -3.063482  #> = Starting epoch 148  #> Test loss: -3.054224  #> = Starting epoch 149  #> Test loss: -3.03651  #> = Starting epoch 150  #> Test loss: -3.108544  #> = Starting epoch 151  #> Test loss: -3.030679  #> = Starting epoch 152  #> Test loss: -3.108284  #> = Starting epoch 153  #> Test loss: -2.976967  #> = Starting epoch 154  #> Test loss: -3.143764  #> = Starting epoch 155  #> Test loss: -3.015362  #> = Starting epoch 156  #> Test loss: -3.141093  #> = Starting epoch 157  #> Test loss: -3.036689  #> = Starting epoch 158  #> Test loss: -3.149788  #> = Starting epoch 159  #> Test loss: -3.083234  #> = Starting epoch 160  #> Test loss: -3.135823  #> = Starting epoch 161  #> Test loss: -3.045046  #> = Starting epoch 162  #> Test loss: -3.131419  #> = Starting epoch 163  #> Test loss: -3.128693  #> = Starting epoch 164  #> Test loss: -3.134316  #> = Starting epoch 165  #> Test loss: -3.11437  #> = Starting epoch 166  #> Test loss: -3.111892  #> = Starting epoch 167  #> Test loss: -3.143839  #> = Starting epoch 168  #> Test loss: -3.14026  #> = Starting epoch 169  #> Test loss: -3.134658  #> = Starting epoch 170  #> Test loss: -3.121037  #> = Starting epoch 171  #> Test loss: -3.150544  #> = Starting epoch 172  #> Test loss: -3.114232  #> = Starting epoch 173  #> Test loss: -3.144404  #> = Starting epoch 174  #> Test loss: -3.082002  #> = Starting epoch 175  #> Test loss: -3.161369  #> = Starting epoch 176  #> Test loss: -3.144196  #> = Starting epoch 177  #> Test loss: -3.100666  #> = Starting epoch 178  #> Test loss: -3.159728  #> = Starting epoch 179  #> Test loss: -3.152668  #> = Starting epoch 180  #> Test loss: -3.148868  #> = Starting epoch 181  #> Test loss: -3.16638  #> = Starting epoch 182  #> Test loss: -3.158306  #> = Starting epoch 183  #> Test loss: -3.178545  #> = Starting epoch 184  #> Test loss: -3.074546  #> = Starting epoch 185  #> Test loss: -2.955466  #> = Starting epoch 186  #> Test loss: -3.137043  #> = Starting epoch 187  #> Test loss: -3.154929  #> = Starting epoch 188  #> Test loss: -3.076744  #> = Starting epoch 189  #> Test loss: -3.126119  #> = Starting epoch 190  #> Test loss: -3.156961  #> = Starting epoch 191  #> Test loss: -3.075396  #> = Starting epoch 192  #> Test loss: -3.158767  #> = Starting epoch 193  #> Test loss: -3.080972  #> = Starting epoch 194  #> Test loss: -2.842314  #> = Starting epoch 195  #> Test loss: -3.170072  #> = Starting epoch 196  #> Test loss: -2.824772  #> = Starting epoch 197  #> Test loss: -2.840855  #> = Starting epoch 198  #> Test loss: -3.065272  #> = Starting epoch 199  #> Test loss: -2.752979  #> = Starting epoch 200  #> Test loss: -3.150843  #> = Starting epoch 201  #> Test loss: -2.925842  #> = Starting epoch 202  #> Test loss: -3.15475  #> = Starting epoch 203  #> Test loss: -2.964058  #> = Starting epoch 204  #> Test loss: -3.089728  #> = Starting epoch 205  #> Test loss: -3.068069  #> = Starting epoch 206  #> Test loss: -3.099001  #> = Starting epoch 207  #> Test loss: -3.132655  #> = Starting epoch 208  #> Test loss: -3.122917  #> = Starting epoch 209  #> Test loss: -3.087993  #> = Starting epoch 210  #> Test loss: -3.154722  #> = Starting epoch 211  #> Test loss: -3.128837  #> = Starting epoch 212  #> Test loss: -3.151212  #> = Starting epoch 213  #> Test loss: -3.16095  #> = Starting epoch 214  #> Test loss: -3.154447  #> = Starting epoch 215  #> Test loss: -3.112319  #> = Starting epoch 216  #> Test loss: -3.168573  #> = Starting epoch 217  #> Test loss: -3.121029  #> = Starting epoch 218  #> Test loss: -3.16887  #> = Starting epoch 219  #> Test loss: -3.179874  #> = Starting epoch 220  #> Test loss: -3.190028  #> = Starting epoch 221  #> Test loss: -3.136508  #> = Starting epoch 222  #> Test loss: -3.191836  #> = Starting epoch 223  #> Test loss: -3.162422  #> = Starting epoch 224  #> Test loss: -3.201498  #> = Starting epoch 225  #> Test loss: -3.121052  #> = Starting epoch 226  #> Test loss: -3.20208  #> = Starting epoch 227  #> Test loss: -3.201113  #> = Starting epoch 228  #> Test loss: -3.13139  #> = Starting epoch 229  #> Test loss: -3.180064  #> = Starting epoch 230  #> Test loss: -3.174343  #> = Starting epoch 231  #> Test loss: -3.201591  #> = Starting epoch 232  #> Test loss: -3.167199  #> = Starting epoch 233  #> Test loss: -3.202939  #> = Starting epoch 234  #> Test loss: -3.189455  #> = Starting epoch 235  #> Test loss: -3.192213  #> = Starting epoch 236  #> Test loss: -3.19517  #> = Starting epoch 237  #> Test loss: -3.190669  #> = Starting epoch 238  #> Test loss: -3.212607  #> = Starting epoch 239  #> Test loss: -3.172253  #> = Starting epoch 240  #> Test loss: -3.197203  #> = Starting epoch 241  #> Test loss: -3.214423  #> = Starting epoch 242  #> Test loss: -3.190518  #> = Starting epoch 243  #> Test loss: -3.202187  #> = Starting epoch 244  #> Test loss: -3.18574  #> = Starting epoch 245  #> Test loss: -3.204652  #> = Starting epoch 246  #> Test loss: -3.200733  #> = Starting epoch 247  #> Test loss: -3.209302  #> = Starting epoch 248  #> Test loss: -3.220846  #> = Starting epoch 249  #> Test loss: -3.196388  #> = Starting epoch 250  #> Test loss: -3.198731  #> = Starting epoch 251  #> Test loss: -3.213181  #> = Starting epoch 252  #> Test loss: -3.208014  #> = Starting epoch 253  #> Test loss: -3.225093  #> = Starting epoch 254  #> Test loss: -3.193242  #> = Starting epoch 255  #> Test loss: -3.183774  #> = Starting epoch 256  #> Test loss: -3.176144 # Generate 1024 samples for each of the first 4 conditioning variables in the test set test_samples <- as_array(generate_from_conditional_flow(   summarizing_flow_model,   1024,   test_set$conditioning[1 : 4, ] )) test_target <- as_array(test_set$target)  par(mfrow = c(4, 2)) for (i in 1 : 4) {   hist(test_samples[, i, 1], main = '', xlab = 'mu', freq = FALSE, breaks = 32, xlim = c(-5, 5))   abline(v = test_target[i, 1], col = 'red')   hist(test_samples[, i, 2], main = '', xlab = 'log(sigma)', freq = FALSE, breaks = 32, xlim = c(-3, 3))   abline(v = test_target[i, 2], col = 'red') } par(mfrow = c(2, 2)) for (i in 1 : 4) {   plot(     test_samples[, i, 1], test_samples[, i, 2], main = '',     xlab = 'mu', ylab = 'log(sigma)', xlim = c(-5, 5), ylim = c(-3, 3)   )   abline(v = test_target[i, 1], col = 'red')   abline(h = test_target[i, 2], col = 'red') }"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"more-complex-conditioning-variables","dir":"Articles","previous_headings":"","what":"More complex conditioning variables","title":"torchflow","text":"summarizing network key using complex conditioning variables. example, can use 2D grid points conditioning variable, processed summarizing network set summary statistics used conditioning variable flow. 2-D grid, convolutional network conventional choice often works well practice. following example generates data using exponential covariance unknown variance length scale 16x16 2-D grid:  can now create summarizing network conditioning variable. convolutional network conventional choice type data. network alternates convolution, ReLU max pooling layers, final adaptive average pooling layer reduce summary statistics fixed size vector dimension 32 (number summary statistics used flow): can now create conditional flow summarizing network: can now train model : can now generate samples trained model:  can also plot samples scatter plot:","code":"n_grid <- 16 x_y_grid <- as.matrix(expand.grid(   x = seq(0, 1, length.out = n_grid),   y = seq(0, 1, length.out = n_grid) )) distances <- torch_tensor(as.matrix(dist(x_y_grid)))  generate_conditional_samples <- function(...) {   n_samples <- 1024   ell <- 0.1 + 1.9 * torch_rand(n_samples)   sigma <- torch_abs(torch_randn(n_samples))    # Generate the conditioning variable   Sigma <- (     torch_square(sigma)$unsqueeze(-1)$unsqueeze(-1) * torch_exp(       -torch_unsqueeze(distances, 1) / ell$unsqueeze(-1)$unsqueeze(-1)     )   )   L <- linalg_cholesky(Sigma)   y_flat <- torch_matmul(L, torch_randn(n_samples, 256, 1))   y <- torch_reshape(y_flat, c(n_samples, n_grid, n_grid))    list(     target = torch_log(torch_stack(list(ell, sigma), 2)),     conditioning = y   ) }  test_set <- generate_conditional_samples() str(test_set) #> List of 2 #>  $ target      :Float [1:1024, 1:2] #>  $ conditioning:Float [1:1024, 1:16, 1:16] par(mfrow = c(2, 2)) for (i in 1 : 4) {   image(as_array(test_set$conditioning[i, , ]), main = '', xlab = 'x', ylab = 'y', asp = 1) } summary_model <- nn_sequential(   # Adds a unit dimension for the channel   nn_unflatten(2, c(1, n_grid)),   nn_conv2d(1, 16, 3, padding = 1),   nn_relu(),   nn_max_pool2d(2),   nn_conv2d(16, 32, 3, padding = 1),   nn_relu(),   # This averages over the grid to produce a vector of summary statistics   nn_adaptive_avg_pool2d(1),   nn_flatten() )  str(summary_model(test_set$conditioning[1 : 10, ])) #> Float [1:10, 1:32] flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 32),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 32),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 32) ) summarizing_flow_model <- nn_summarizing_conditional_flow(summary_model, flow_model) train_conditional_flow(   summarizing_flow_model,   generate_conditional_samples,   n_epochs = 128,   batch_size = 1024,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(summarizing_flow_model(test_set$target, test_set$conditioning)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 1.085365  #> = Starting epoch 2  #> Test loss: 0.9748415  #> = Starting epoch 3  #> Test loss: 0.8902185  #> = Starting epoch 4  #> Test loss: 0.8280191  #> = Starting epoch 5  #> Test loss: 0.7844825  #> = Starting epoch 6  #> Test loss: 0.7498901  #> = Starting epoch 7  #> Test loss: 0.7014447  #> = Starting epoch 8  #> Test loss: 0.6426057  #> = Starting epoch 9  #> Test loss: 0.5741165  #> = Starting epoch 10  #> Test loss: 0.500012  #> = Starting epoch 11  #> Test loss: 0.4249643  #> = Starting epoch 12  #> Test loss: 0.3491445  #> = Starting epoch 13  #> Test loss: 0.2753627  #> = Starting epoch 14  #> Test loss: 0.2070841  #> = Starting epoch 15  #> Test loss: 0.1480916  #> = Starting epoch 16  #> Test loss: 0.08360118  #> = Starting epoch 17  #> Test loss: -0.00555855  #> = Starting epoch 18  #> Test loss: -0.1011122  #> = Starting epoch 19  #> Test loss: -0.1880344  #> = Starting epoch 20  #> Test loss: -0.2311422  #> = Starting epoch 21  #> Test loss: -0.2728342  #> = Starting epoch 22  #> Test loss: -0.3348165  #> = Starting epoch 23  #> Test loss: -0.3705704  #> = Starting epoch 24  #> Test loss: -0.4184242  #> = Starting epoch 25  #> Test loss: -0.3201386  #> = Starting epoch 26  #> Test loss: -0.4939158  #> = Starting epoch 27  #> Test loss: -0.5213039  #> = Starting epoch 28  #> Test loss: -0.7229104  #> = Starting epoch 29  #> Test loss: -0.5580709  #> = Starting epoch 30  #> Test loss: -0.6797779  #> = Starting epoch 31  #> Test loss: -0.7578655  #> = Starting epoch 32  #> Test loss: -0.7551997  #> = Starting epoch 33  #> Test loss: -0.8471733  #> = Starting epoch 34  #> Test loss: -0.8607615  #> = Starting epoch 35  #> Test loss: -0.8831026  #> = Starting epoch 36  #> Test loss: -0.9237244  #> = Starting epoch 37  #> Test loss: -0.9639399  #> = Starting epoch 38  #> Test loss: -1.036772  #> = Starting epoch 39  #> Test loss: -1.008096  #> = Starting epoch 40  #> Test loss: -1.100039  #> = Starting epoch 41  #> Test loss: -1.082681  #> = Starting epoch 42  #> Test loss: -1.160505  #> = Starting epoch 43  #> Test loss: -1.187675  #> = Starting epoch 44  #> Test loss: -1.253604  #> = Starting epoch 45  #> Test loss: -1.262604  #> = Starting epoch 46  #> Test loss: -1.186344  #> = Starting epoch 47  #> Test loss: -1.251473  #> = Starting epoch 48  #> Test loss: -1.338266  #> = Starting epoch 49  #> Test loss: -1.228902  #> = Starting epoch 50  #> Test loss: -1.404902  #> = Starting epoch 51  #> Test loss: -1.407388  #> = Starting epoch 52  #> Test loss: -1.440488  #> = Starting epoch 53  #> Test loss: -1.343027  #> = Starting epoch 54  #> Test loss: -1.469395  #> = Starting epoch 55  #> Test loss: -1.521472  #> = Starting epoch 56  #> Test loss: -1.455972  #> = Starting epoch 57  #> Test loss: -1.512094  #> = Starting epoch 58  #> Test loss: -1.452621  #> = Starting epoch 59  #> Test loss: -1.595996  #> = Starting epoch 60  #> Test loss: -1.461387  #> = Starting epoch 61  #> Test loss: -1.6045  #> = Starting epoch 62  #> Test loss: -1.578959  #> = Starting epoch 63  #> Test loss: -1.652403  #> = Starting epoch 64  #> Test loss: -1.609594  #> = Starting epoch 65  #> Test loss: -1.64576  #> = Starting epoch 66  #> Test loss: -1.641537  #> = Starting epoch 67  #> Test loss: -1.726631  #> = Starting epoch 68  #> Test loss: -1.705062  #> = Starting epoch 69  #> Test loss: -1.700799  #> = Starting epoch 70  #> Test loss: -1.736501  #> = Starting epoch 71  #> Test loss: -1.743962  #> = Starting epoch 72  #> Test loss: -1.774387  #> = Starting epoch 73  #> Test loss: -1.722433  #> = Starting epoch 74  #> Test loss: -1.822642  #> = Starting epoch 75  #> Test loss: -1.787688  #> = Starting epoch 76  #> Test loss: -1.825814  #> = Starting epoch 77  #> Test loss: -1.853987  #> = Starting epoch 78  #> Test loss: -1.787948  #> = Starting epoch 79  #> Test loss: -1.878468  #> = Starting epoch 80  #> Test loss: -1.898285  #> = Starting epoch 81  #> Test loss: -1.836211  #> = Starting epoch 82  #> Test loss: -1.949654  #> = Starting epoch 83  #> Test loss: -1.904361  #> = Starting epoch 84  #> Test loss: -1.880097  #> = Starting epoch 85  #> Test loss: -1.963178  #> = Starting epoch 86  #> Test loss: -1.984369  #> = Starting epoch 87  #> Test loss: -1.982115  #> = Starting epoch 88  #> Test loss: -2.007647  #> = Starting epoch 89  #> Test loss: -2.002118  #> = Starting epoch 90  #> Test loss: -1.967696  #> = Starting epoch 91  #> Test loss: -2.00314  #> = Starting epoch 92  #> Test loss: -2.03957  #> = Starting epoch 93  #> Test loss: -2.030533  #> = Starting epoch 94  #> Test loss: -2.026528  #> = Starting epoch 95  #> Test loss: -1.961485  #> = Starting epoch 96  #> Test loss: -1.973998  #> = Starting epoch 97  #> Test loss: -2.052812  #> = Starting epoch 98  #> Test loss: -2.076638  #> = Starting epoch 99  #> Test loss: -2.004556  #> = Starting epoch 100  #> Test loss: -2.001481  #> = Starting epoch 101  #> Test loss: -2.090852  #> = Starting epoch 102  #> Test loss: -2.08062  #> = Starting epoch 103  #> Test loss: -2.120534  #> = Starting epoch 104  #> Test loss: -2.103983  #> = Starting epoch 105  #> Test loss: -2.128767  #> = Starting epoch 106  #> Test loss: -2.120615  #> = Starting epoch 107  #> Test loss: -2.129828  #> = Starting epoch 108  #> Test loss: -2.118576  #> = Starting epoch 109  #> Test loss: -2.059935  #> = Starting epoch 110  #> Test loss: -1.956183  #> = Starting epoch 111  #> Test loss: -1.90592  #> = Starting epoch 112  #> Test loss: -2.135971  #> = Starting epoch 113  #> Test loss: -2.067734  #> = Starting epoch 114  #> Test loss: -1.984683  #> = Starting epoch 115  #> Test loss: -2.118106  #> = Starting epoch 116  #> Test loss: -2.027083  #> = Starting epoch 117  #> Test loss: -2.065608  #> = Starting epoch 118  #> Test loss: -2.132512  #> = Starting epoch 119  #> Test loss: -1.94119  #> = Starting epoch 120  #> Test loss: -2.087802  #> = Starting epoch 121  #> Test loss: -2.119181  #> = Starting epoch 122  #> Test loss: -1.963944  #> = Starting epoch 123  #> Test loss: -2.20462  #> = Starting epoch 124  #> Test loss: -2.102631  #> = Starting epoch 125  #> Test loss: -2.135066  #> = Starting epoch 126  #> Test loss: -2.140841  #> = Starting epoch 127  #> Test loss: -2.128747  #> = Starting epoch 128  #> Test loss: -2.186756 test_samples <- as_array(generate_from_conditional_flow(summarizing_flow_model, 1024, test_set$conditioning[1 : 4, , , drop = FALSE])) str(test_samples) #>  num [1:1024, 1:4, 1:2] -0.0567 0.4361 -1.3117 -0.7474 0.4847 ...  test_target <- as_array(test_set$target)  par(mfrow = c(4, 2)) for (i in 1 : 4) {   hist(exp(test_samples[, i, 1]), main = '', xlab = 'ell', freq = FALSE, breaks = 32, xlim = c(0, 2))   abline(v = exp(test_target[i, 1]), col = 'red')   hist(exp(test_samples[, i, 2]), main = '', xlab = 'sigma', freq = FALSE, breaks = 32, xlim = c(0, 3))   abline(v = exp(test_target[i, 2]), col = 'red') } par(mfrow = c(2, 2)) for (i in 1 : 4) {   plot(exp(test_samples[, i, 1]), exp(test_samples[, i, 2]), main = '', xlab = 'ell', ylab = 'sigma', xlim = c(0, 2), ylim = c(0, 3))   abline(v = exp(test_target[i, 1]), col = 'red')   abline(h = exp(test_target[i, 2]), col = 'red') }"},{"path":"https://mbertolacci.github.io/torchflow/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael Bertolacci. Author, maintainer.","code":""},{"path":"https://mbertolacci.github.io/torchflow/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bertolacci M (2025). torchflow: Conditional Normalizing Flows. R package version 0.0.1, https://mbertolacci.github.io/torchflow/.","code":"@Manual{,   title = {torchflow: Conditional Normalizing Flows},   author = {Michael Bertolacci},   year = {2025},   note = {R package version 0.0.1},   url = {https://mbertolacci.github.io/torchflow/}, }"},{"path":"https://mbertolacci.github.io/torchflow/index.html","id":"torchflow","dir":"","previous_headings":"","what":"Conditional Normalizing Flows","title":"Conditional Normalizing Flows","text":"Normalizing flows R using torch package. can install package source using devtools package: {r} devtools::install_github('mbertolacci/torchflow') Go read vignette package website learn : torchflow. details come","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward KL Loss — forward_kl_loss","title":"Forward KL Loss — forward_kl_loss","text":"Compute forward KL loss flow model.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward KL Loss — forward_kl_loss","text":"","code":"forward_kl_loss(input)"},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward KL Loss — forward_kl_loss","text":"input output flow model, tensor dimensions [batch1, ..., batchN, d] batch1, ..., batchN dimensions batch d dimension input. must also attribute log_jacobian containing log determinant Jacobian transformation.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forward KL Loss — forward_kl_loss","text":"","code":"library(torch) flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 0),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 0) ) x <- torch_randn(10, 2) y <- flow_model(x) loss <- forward_kl_loss(y)"},{"path":"https://mbertolacci.github.io/torchflow/reference/generate_from_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate samples from a conditional flow model — generate_from_conditional_flow","title":"Generate samples from a conditional flow model — generate_from_conditional_flow","text":"function generates samples conditional flow model. conditioning provided, n_samples_per_batch samples generated batch conditioning variable. conditioning provided, n_samples_per_batch samples generated.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/generate_from_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate samples from a conditional flow model — generate_from_conditional_flow","text":"","code":"generate_from_conditional_flow(model, n_samples_per_batch, conditioning)"},{"path":"https://mbertolacci.github.io/torchflow/reference/generate_from_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate samples from a conditional flow model — generate_from_conditional_flow","text":"model conditional flow model. n_samples_per_batch number samples generate batch conditioning variable, total number samples conditioning provided. conditioning conditioning variable, torch tensor dimensions [batch, ...] batch dimension batch ... dimensions conditioning variable.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Affine Coupling Block — nn_affine_coupling_block","title":"Affine Coupling Block — nn_affine_coupling_block","text":"affine coupling block conditional flow inheriting nn_conditional_flow() applies following transformation input.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Affine Coupling Block — nn_affine_coupling_block","text":"","code":"nn_affine_coupling_block(   input_size,   conditioning_size = 0,   left_size = as.integer(input_size%/%2),   f_scale,   f_shift,   g_scale,   g_shift,   soft_clamp = 1.9 )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Affine Coupling Block — nn_affine_coupling_block","text":"input_size dimension input. input tensor dimensions [batch_size, input_size], just [input_size] batch dimension. conditioning_size dimension conditioning input, batch dimensions input. left_size dimension left part input (split \\(x_1\\) equations ). f_scale function \\(f_\\text{scale}\\) equations . , following parameters, default conditional multi-layer perceptron (MLP); see nn_conditional_mlp(). must inherit nn_conditional(). f_shift function \\(f_\\text{shift}\\) equations ; see . g_scale function \\(g_\\text{scale}\\) equations ; see . g_shift function \\(g_\\text{shift}\\) equations ; see . soft_clamp soft clamp value scale parameters.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Affine Coupling Block — nn_affine_coupling_block","text":"Let \\(x = (x_1, x_2)\\) split input two parts, let \\(u\\) conditioning input. forward transformation given : $$   y_1 = x_1 \\exp(f_\\text{scale}(x_2, u)) + f_\\text{shift}(x_2, u)   y_2 = x_2 \\exp(g_\\text{scale}(y_1, u)) + g_\\text{shift}(y_1, u) $$ inverse transformation given : $$   x_1 = y_1 \\exp(g_\\text{scale}(y_2, u)) + g_\\text{shift}(y_2, u)   x_2 = y_2 \\exp(f_\\text{scale}(x_1, u)) + f_\\text{shift}(x_1, u) $$ log determinant Jacobian transformation given : $$   \\log | \\det \\frac{\\partial y}{\\partial x} |    = \\sum_{=1}^2 f_\\text{scale}(x_i, u) + g_\\text{scale}(y_i, u) $$ performing multiple transformations sequence, can construct complex normalizing flow capable modeling complicated conditional distributions. pair transformations, dimensions input permuted using nn_permutation_flow().","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Affine Coupling Block — nn_affine_coupling_block","text":"","code":"library(torch) # Coupling block used on its own with no conditioning flow_model <- nn_affine_coupling_block(2, 0) x <- torch_randn(10, 2) y <- flow_model(x) # y will be a tensor of dimensions [10, 2] x_recovered <- flow_model$reverse(y) # x_recovered will be a tensor of dimensions [10, 2] # and numerically close to the original x  # Coupling block used with conditioning flow_model <- nn_affine_coupling_block(2, 4) x <- torch_randn(10, 2) u <- torch_randn(10, 4) y <- flow_model(x, u) # y will be a tensor of dimensions [10, 2] x_recovered <- flow_model$reverse(y, u) # x_recovered will be a tensor of dimensions [10, 2] # and numerically close to the original x  # Coupling block used as part of a more complex flow model flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 4),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 4) ) y <- flow_model(x, u)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Module — nn_conditional","title":"Conditional Module — nn_conditional","text":"conditional module module takes additional conditioning input forward pass.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Module — nn_conditional","text":"","code":"nn_conditional()"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional.html","id":"forward-method","dir":"Reference","previous_headings":"","what":"Forward method","title":"Conditional Module — nn_conditional","text":"forward method take two arguments, input conditioning, return output. Example:","code":"forward = function(input, conditioning) {   output <- ...   output }"},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Normalizing Flow — nn_conditional_flow","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"conditional normalizing flow normalizing flow takes additional conditioning input. module provides base class conditional normalizing flows.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"","code":"nn_conditional_flow()"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"base class, nn_conditional_flow, abstract class provides forward reverse method, well dimension method. Subclasses created torch::nn_module() implement methods. class subclass torch::nn_module(), inherits methods semantics.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"forward-method","dir":"Reference","previous_headings":"","what":"Forward method","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"forward method return output log determinant Jacobian attribute log_jacobian. Example:","code":"forward = function(input, conditioning) {   output <- ...   attr(output, 'log_jacobian') <- ...   output }"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"reverse-method","dir":"Reference","previous_headings":"","what":"Reverse method","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"reverse method return inverse output, need implement log determinant. Example:","code":"reverse = function(input, conditioning) {   output <- ...   output }"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"dimension-method","dir":"Reference","previous_headings":"","what":"Dimension method","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"dimension method return dimension input output flow. Example:","code":"dimension = function() {   return(2) }"},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Multilayer Perceptron — nn_conditional_mlp","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"conditional multilayer perceptron multilayer perceptron takes additional conditioning input. inherits nn_conditional(); normalizing flow. practice, regular input conditioning input concatenated passed MLP.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"","code":"nn_conditional_mlp(   input_size,   conditioning_size,   output_size,   layer_sizes = c(128, 128),   activation = nn_relu )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"input_size size input MLP. conditioning_size size conditioning input MLP. output_size size output MLP. layer_sizes vector integers specifying number neurons layer. can NULL, case single linear layer used. activation activation function use layer.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"","code":"library(torch) mlp <- nn_conditional_mlp(10, 5, 1) input <- torch_randn(10) conditioning <- torch_randn(5) output <- mlp(input, conditioning)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation Flow — nn_permutation_flow","title":"Permutation Flow — nn_permutation_flow","text":"permutation flow conditional flow inheriting nn_conditional_flow() permutes input dimensions. permutation fixed random permutation initialization change. log Jacobian zero since simple reordering input dimensions.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Flow — nn_permutation_flow","text":"","code":"nn_permutation_flow(input_size)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Flow — nn_permutation_flow","text":"input_size size input flow.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Permutation Flow — nn_permutation_flow","text":"","code":"library(torch) # Use on its own permutation_flow <- nn_permutation_flow(10) input <- torch_randn(10) output <- permutation_flow(input) # Use in a more complex conditional flow flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(10, 5),   nn_permutation_flow(10),   nn_affine_coupling_block(10, 5) )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Sequential Conditional Flow — nn_sequential_conditional_flow","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"sequential conditional flow conditional flow inheriting nn_conditional_flow() applies sequence conditional flows, passing conditioning input flow. analog torch::nn_sequential() conditional flows.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"","code":"nn_sequential_conditional_flow(...)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"... sequence conditional flows, list conditional flows.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"","code":"flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(10, 5),   nn_permutation_flow(10),   nn_affine_coupling_block(10, 5) )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"summarizing conditional flow conditional flow summarizes conditioning input using summary model. inherits nn_conditional_flow(), requires summary_model flow_model initializer. forward pass, summary model applied conditioning input, result passed flow model. can used reduce dimensionality conditioning input.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"","code":"nn_summarizing_conditional_flow(summary_model, flow_model)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"summary_model torch::nn_module() summarizes conditioning input. flow_model torch::nn_module() conditional flow.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"forward() method optional summary argument. summary provided, used conditioning input flow model, skipping application summary model. can used speed forward pass conditioning input used multiple times. class also provides summarize() method can used compute summary conditioning input outside forward pass.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"","code":"library(torch) summary_model <- nn_sequential(   nn_linear(10, 5),   nn_relu() ) flow_model <- nn_affine_coupling_block(10, 5) summarizing_flow <- nn_summarizing_conditional_flow(summary_model, flow_model)"},{"path":"https://mbertolacci.github.io/torchflow/reference/torchflow-package.html","id":null,"dir":"Reference","previous_headings":"","what":"torchflow: Conditional Normalizing Flows — torchflow-package","title":"torchflow: Conditional Normalizing Flows — torchflow-package","text":"package fit conditional normalizing flows.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/torchflow-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"torchflow: Conditional Normalizing Flows — torchflow-package","text":"Maintainer: Michael Bertolacci m.bertolacci@gmail.com (ORCID)","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Train a conditional flow model — train_conditional_flow","title":"Train a conditional flow model — train_conditional_flow","text":"Method train conditional flow model. basic training loop following steps:","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train a conditional flow model — train_conditional_flow","text":"","code":"train_conditional_flow(   model,   generate,   optimizer = torch::optim_adam,   n_epochs = 128,   batch_size = 32,   after_epoch = NULL,   verbose = TRUE,   ... )"},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train a conditional flow model — train_conditional_flow","text":"model conditional flow model inheriting nn_conditional_flow(). generate function generates batch target conditioning samples; see details. passed current epoch number argument. optimizer optimizer use, e.g. torch::optim_adam(). n_epochs number epochs train . batch_size batch size. after_epoch function call epoch. verbose Whether print progress. ... Additional arguments pass optimizer.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Train a conditional flow model — train_conditional_flow","text":"training algorithm follows. epoch: Generate (using generate) batch target conditioning samples. Loop batches epoch, performing gradient descent step batch. batches processed order generated samples. Call after_epoch (provided) current epoch generated samples. can used print test loss tasks. generate function (called current epoch number argument) return list following elements: target: array(), matrix(), torch::torch_tensor() target samples. conditioning: optional array(), matrix(), torch::torch_tensor() conditioning samples. returning torch_tensor() objects, take care device model. generated samples choice user. generate new samples epoch, share samples across epochs (noting model may overfit case). latter case, good permute order samples epoch. training may stopped early. original model object modified place.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train a conditional flow model — train_conditional_flow","text":"","code":"library(torch) model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(input_size = 2),   nn_permutation_flow(input_size = 2),   nn_affine_coupling_block(input_size = 2) ) generate <- function(epoch) {   list(target = 2 + torch_randn(1024, 2)) } # In practice, the number of epochs should be larger train_conditional_flow(model, generate, n_epochs = 2) #> = Starting epoch 1  #> = Starting epoch 2"}]
