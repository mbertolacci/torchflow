[{"path":"https://mbertolacci.github.io/torchflow/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 torchflow authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"torchflow: Normalizing Flows using R torch","text":"Give mathematical details","code":""},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"creating-and-sampling-from-a-flow","dir":"Articles","previous_headings":"","what":"Creating and sampling from a flow","title":"torchflow: Normalizing Flows using R torch","text":"simple two parameter normalising flow can created follows: flow five layers, alternate affine coupling blocks permutation flows. flow just standard torch nn_module can used usual way: first dimension input acts batch dimension, flow can used generate multiple samples . code actually implements sampling distribution represented flow. can also done directly using generate_from_conditional_flow function:","code":"flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2),   nn_permutation_flow(2),   nn_affine_coupling_block(2),   nn_permutation_flow(2),   nn_affine_coupling_block(2) ) x <- torch_randn(5, 2) flow_model(x) #> torch_tensor #>  0.7620  1.2287 #>  0.7782  0.1347 #> -0.3300 -1.9583 #>  0.2246  0.6583 #> -0.4207 -0.2339 #> [ CPUFloatType{5,2} ][ grad_fn = <CatBackward0> ] generate_from_conditional_flow(flow_model, 5) #> torch_tensor #> -0.0049 -1.2907 #> -0.6298  1.8716 #> -0.9137  0.9934 #> -1.2454  2.2633 #>  0.3215 -1.1245 #> [ CPUFloatType{5,2} ][ grad_fn = <ReshapeAliasBackward0> ]"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"conditional-flow","dir":"Articles","previous_headings":"","what":"Conditional flow","title":"torchflow: Normalizing Flows using R torch","text":"conditional flow takes additional input, conditioning variable, can used condition samples additional information. flow therefore encodes conditional distribution. following code creates conditional flow architecture unconditional flow defined additional conditioning variable dimension 3: can sample flow given conditioning variable follows: can also batch conditioning variables:","code":"conditioned_flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 3),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 3) ) conditioning <- torch_randn(3) generate_from_conditional_flow(conditioned_flow_model, 5, conditioning) #> torch_tensor #>  0.8864 -0.4017 #> -2.3727 -0.4412 #> -1.2729  0.7148 #>  0.8890 -0.3691 #> -0.0843  1.4628 #> [ CPUFloatType{5,2} ][ grad_fn = <ReshapeAliasBackward0> ] conditioning <- torch_randn(8, 3) generate_from_conditional_flow(conditioned_flow_model, 5, conditioning) #> torch_tensor #> (1,.,.) =  #>  -0.5824 -0.0515 #>  -0.9207  0.8802 #>   1.4992  0.8227 #>   1.5624  0.6627 #>   0.4472  0.9105 #>   0.6486 -1.1479 #>  -1.1329  1.1935 #>   0.4598  0.3404 #>  #> (2,.,.) =  #>   0.0424  0.3839 #>  -1.3179 -1.0277 #>   0.4484 -0.7403 #>  -1.2641 -0.6590 #>   1.1145  0.1883 #>   0.3371 -0.1862 #>   1.7885  0.0867 #>  -0.0099 -0.0765 #>  #> (3,.,.) =  #>   0.3426  1.5401 #>   0.5718 -0.1705 #>   0.6092  1.1342 #>   0.6860 -1.2105 #>  -0.1089 -0.1245 #>  -2.2699  0.2824 #>   0.7406 -0.6958 #>  -0.4172 -0.5103 #>  #> ... [the output was truncated (use n=-1 to disable)] #> [ CPUFloatType{5,8,2} ][ grad_fn = <ReshapeAliasBackward0> ]"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"training-an-unconditional-flow","dir":"Articles","previous_headings":"","what":"Training an unconditional flow","title":"torchflow: Normalizing Flows using R torch","text":"flows randomly initialised, samples follow interesting distribution. can instead train flow follow given distribution using samples distribution. Let us train flow match following distribution σ∼N+(0,1)\\sigma \\sim N^+(0, 1), μ∼N(0,σ2)\\mu \\sim N(0, \\sigma^2), N+(0,1)N^+(0, 1) half normal distribution mean 0 standard deviation 1. can generate samples distribution follows: function can used train flow match distribution using train_conditional_flow function: looks though test loss converged. can sample trained flow follows compare samples test set:  looks like reasonable approximation target distribution. can also look marginal histograms:  look okay.","code":"generate_samples <- function(...) {   n_samples <- 1024   sigma <- torch_abs(torch_randn(n_samples))   mu <- torch_randn(n_samples) * sigma   list(target = torch_stack(list(mu, sigma), 2)) }  generate_samples() #> $target #> torch_tensor #> -2.0356e+00  2.1848e+00 #> -3.7028e-01  1.0502e+00 #>  2.0638e-01  3.1134e-01 #> -2.4274e-01  2.6957e-01 #>  9.6417e-01  1.9926e+00 #>  2.8992e-05  7.1465e-05 #>  2.9896e+00  2.1187e+00 #> -7.3388e-01  1.1435e+00 #>  4.0176e-01  1.3425e+00 #>  5.4139e-01  1.3120e+00 #> -1.5147e-01  4.8527e-01 #> -9.3724e-01  4.3965e-01 #>  1.3770e-01  6.4735e-02 #> -3.9224e-01  8.1522e-01 #> -1.6318e-01  2.3478e-01 #>  8.5131e-02  1.9225e-01 #> -3.9256e-01  4.5994e-01 #> -1.0516e-01  2.0527e-01 #> -4.7685e-01  5.4539e-01 #> -3.2769e-01  3.9091e-01 #>  2.1266e-01  1.0014e-01 #> -4.2534e-02  3.2324e-02 #> -7.7913e-01  9.7084e-01 #>  2.3862e-01  1.0406e+00 #>  3.5122e-02  1.4570e-01 #>  6.5700e-01  8.9683e-01 #> -2.0690e+00  1.9685e+00 #>  1.4820e+00  8.5359e-01 #> -6.9395e-01  1.1226e+00 #>  1.9841e+00  1.8730e+00 #> ... [the output was truncated (use n=-1 to disable)] #> [ CPUFloatType{1024,2} ] test_set <- generate_samples() train_conditional_flow(   flow_model,   generate_samples,   n_epochs = 32,   batch_size = 256,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(flow_model(test_set$target)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 0.5469023  #> = Starting epoch 2  #> Test loss: 0.2499641  #> = Starting epoch 3  #> Test loss: 0.1619004  #> = Starting epoch 4  #> Test loss: 0.1500571  #> = Starting epoch 5  #> Test loss: 0.0622198  #> = Starting epoch 6  #> Test loss: 0.02612031  #> = Starting epoch 7  #> Test loss: -0.007688463  #> = Starting epoch 8  #> Test loss: -0.04603517  #> = Starting epoch 9  #> Test loss: -0.1078135  #> = Starting epoch 10  #> Test loss: -0.1059128  #> = Starting epoch 11  #> Test loss: -0.1240132  #> = Starting epoch 12  #> Test loss: -0.111249  #> = Starting epoch 13  #> Test loss: -0.1626916  #> = Starting epoch 14  #> Test loss: -0.1532276  #> = Starting epoch 15  #> Test loss: -0.1728297  #> = Starting epoch 16  #> Test loss: -0.1788442  #> = Starting epoch 17  #> Test loss: -0.1835951  #> = Starting epoch 18  #> Test loss: -0.1981164  #> = Starting epoch 19  #> Test loss: -0.1863381  #> = Starting epoch 20  #> Test loss: -0.2018799  #> = Starting epoch 21  #> Test loss: -0.1780871  #> = Starting epoch 22  #> Test loss: -0.1862966  #> = Starting epoch 23  #> Test loss: -0.1865796  #> = Starting epoch 24  #> Test loss: -0.2016056  #> = Starting epoch 25  #> Test loss: -0.2185627  #> = Starting epoch 26  #> Test loss: -0.1756115  #> = Starting epoch 27  #> Test loss: -0.1658197  #> = Starting epoch 28  #> Test loss: -0.1506733  #> = Starting epoch 29  #> Test loss: -0.2066042  #> = Starting epoch 30  #> Test loss: -0.2044319  #> = Starting epoch 31  #> Test loss: -0.2119279  #> = Starting epoch 32  #> Test loss: -0.1880534 test_samples <- generate_from_conditional_flow(flow_model, 1024) plot(as_array(test_set$target), xlab = 'mu', ylab = 'sigma') points(as_array(test_samples), col = 'red') par(mfrow = c(2, 2)) hist(as_array(test_set$target[, 1]), main = 'Target', xlab = 'mu', freq = FALSE, breaks = 32) hist(as_array(test_samples[, 1]), main = 'Samples', xlab = 'mu', freq = FALSE, breaks = 32) hist(as_array(test_set$target[, 2]), main = 'Target', xlab = 'sigma', freq = FALSE, breaks = 32) hist(as_array(test_samples[, 2]), main = 'Samples', xlab = 'sigma', freq = FALSE, breaks = 32)"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"training-a-conditional-flow","dir":"Articles","previous_headings":"","what":"Training a conditional flow","title":"torchflow: Normalizing Flows using R torch","text":"process training conditional flow unconditional flow, except generate function now also returns conditioning variable. Let’s add conditioning variable, y∼N(μ,σ2)y \\sim N(\\mu, \\sigma^2), four replicates: can generate samples trained flow follows, now samples conditioned values yy:  can also plot samples scatter plot:  Compare MCMC","code":"generate_conditional_samples <- function(...) {   n_samples <- 1024   sigma <- torch_abs(torch_randn(n_samples))   mu <- torch_randn(n_samples) * sigma   y <- torch_unsqueeze(mu, 2) + torch_randn(n_samples, 4) * torch_unsqueeze(sigma, 2)   list(     target = torch_stack(list(mu, sigma), 2),     conditioning = y   ) }  conditioning_flow <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 4),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 4),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 4) )  test_set <- generate_conditional_samples() str(test_set) #> List of 2 #>  $ target      :Float [1:1024, 1:2] #>  $ conditioning:Float [1:1024, 1:4] train_conditional_flow(   conditioning_flow,   generate_conditional_samples,   n_epochs = 128,   batch_size = 256,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(conditioning_flow(test_set$target, test_set$conditioning)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 0.01622194  #> = Starting epoch 2  #> Test loss: -0.1600565  #> = Starting epoch 3  #> Test loss: -0.433969  #> = Starting epoch 4  #> Test loss: -0.6365002  #> = Starting epoch 5  #> Test loss: -0.6937561  #> = Starting epoch 6  #> Test loss: -0.7636083  #> = Starting epoch 7  #> Test loss: -0.9626853  #> = Starting epoch 8  #> Test loss: -1.137826  #> = Starting epoch 9  #> Test loss: -1.25346  #> = Starting epoch 10  #> Test loss: -1.341025  #> = Starting epoch 11  #> Test loss: -1.354002  #> = Starting epoch 12  #> Test loss: -1.411517  #> = Starting epoch 13  #> Test loss: -1.455572  #> = Starting epoch 14  #> Test loss: -1.492022  #> = Starting epoch 15  #> Test loss: -1.52513  #> = Starting epoch 16  #> Test loss: -1.549721  #> = Starting epoch 17  #> Test loss: -1.547466  #> = Starting epoch 18  #> Test loss: -1.596376  #> = Starting epoch 19  #> Test loss: -1.605451  #> = Starting epoch 20  #> Test loss: -1.622974  #> = Starting epoch 21  #> Test loss: -1.628625  #> = Starting epoch 22  #> Test loss: -1.64203  #> = Starting epoch 23  #> Test loss: -1.665685  #> = Starting epoch 24  #> Test loss: -1.634296  #> = Starting epoch 25  #> Test loss: -1.668058  #> = Starting epoch 26  #> Test loss: -1.69806  #> = Starting epoch 27  #> Test loss: -1.716853  #> = Starting epoch 28  #> Test loss: -1.70732  #> = Starting epoch 29  #> Test loss: -1.744259  #> = Starting epoch 30  #> Test loss: -1.769782  #> = Starting epoch 31  #> Test loss: -1.769986  #> = Starting epoch 32  #> Test loss: -1.761097  #> = Starting epoch 33  #> Test loss: -1.78656  #> = Starting epoch 34  #> Test loss: -1.797829  #> = Starting epoch 35  #> Test loss: -1.786608  #> = Starting epoch 36  #> Test loss: -1.823979  #> = Starting epoch 37  #> Test loss: -1.830046  #> = Starting epoch 38  #> Test loss: -1.831178  #> = Starting epoch 39  #> Test loss: -1.822447  #> = Starting epoch 40  #> Test loss: -1.846764  #> = Starting epoch 41  #> Test loss: -1.864379  #> = Starting epoch 42  #> Test loss: -1.852462  #> = Starting epoch 43  #> Test loss: -1.820471  #> = Starting epoch 44  #> Test loss: -1.851835  #> = Starting epoch 45  #> Test loss: -1.900735  #> = Starting epoch 46  #> Test loss: -1.881277  #> = Starting epoch 47  #> Test loss: -1.881364  #> = Starting epoch 48  #> Test loss: -1.910052  #> = Starting epoch 49  #> Test loss: -1.920465  #> = Starting epoch 50  #> Test loss: -1.914604  #> = Starting epoch 51  #> Test loss: -1.947809  #> = Starting epoch 52  #> Test loss: -1.929116  #> = Starting epoch 53  #> Test loss: -1.927137  #> = Starting epoch 54  #> Test loss: -1.959081  #> = Starting epoch 55  #> Test loss: -1.955928  #> = Starting epoch 56  #> Test loss: -1.899044  #> = Starting epoch 57  #> Test loss: -1.941839  #> = Starting epoch 58  #> Test loss: -1.952915  #> = Starting epoch 59  #> Test loss: -1.95999  #> = Starting epoch 60  #> Test loss: -1.932041  #> = Starting epoch 61  #> Test loss: -1.925941  #> = Starting epoch 62  #> Test loss: -1.94037  #> = Starting epoch 63  #> Test loss: -1.96522  #> = Starting epoch 64  #> Test loss: -1.953818  #> = Starting epoch 65  #> Test loss: -1.938549  #> = Starting epoch 66  #> Test loss: -1.944006  #> = Starting epoch 67  #> Test loss: -1.961961  #> = Starting epoch 68  #> Test loss: -1.94028  #> = Starting epoch 69  #> Test loss: -1.974152  #> = Starting epoch 70  #> Test loss: -1.979357  #> = Starting epoch 71  #> Test loss: -1.982947  #> = Starting epoch 72  #> Test loss: -1.96897  #> = Starting epoch 73  #> Test loss: -1.97713  #> = Starting epoch 74  #> Test loss: -1.999575  #> = Starting epoch 75  #> Test loss: -1.992031  #> = Starting epoch 76  #> Test loss: -1.999038  #> = Starting epoch 77  #> Test loss: -1.993555  #> = Starting epoch 78  #> Test loss: -1.977866  #> = Starting epoch 79  #> Test loss: -2.0021  #> = Starting epoch 80  #> Test loss: -1.992567  #> = Starting epoch 81  #> Test loss: -1.983955  #> = Starting epoch 82  #> Test loss: -2.008642  #> = Starting epoch 83  #> Test loss: -2.019134  #> = Starting epoch 84  #> Test loss: -2.012392  #> = Starting epoch 85  #> Test loss: -1.98646  #> = Starting epoch 86  #> Test loss: -1.977364  #> = Starting epoch 87  #> Test loss: -2.000924  #> = Starting epoch 88  #> Test loss: -2.011215  #> = Starting epoch 89  #> Test loss: -2.023432  #> = Starting epoch 90  #> Test loss: -1.993426  #> = Starting epoch 91  #> Test loss: -1.982684  #> = Starting epoch 92  #> Test loss: -1.963305  #> = Starting epoch 93  #> Test loss: -1.95559  #> = Starting epoch 94  #> Test loss: -2.009637  #> = Starting epoch 95  #> Test loss: -1.989193  #> = Starting epoch 96  #> Test loss: -1.971342  #> = Starting epoch 97  #> Test loss: -1.988139  #> = Starting epoch 98  #> Test loss: -2.002418  #> = Starting epoch 99  #> Test loss: -2.000582  #> = Starting epoch 100  #> Test loss: -2.017789  #> = Starting epoch 101  #> Test loss: -2.044932  #> = Starting epoch 102  #> Test loss: -2.039354  #> = Starting epoch 103  #> Test loss: -2.026986  #> = Starting epoch 104  #> Test loss: -2.018082  #> = Starting epoch 105  #> Test loss: -1.980145  #> = Starting epoch 106  #> Test loss: -2.012449  #> = Starting epoch 107  #> Test loss: -1.972013  #> = Starting epoch 108  #> Test loss: -1.997239  #> = Starting epoch 109  #> Test loss: -2.009499  #> = Starting epoch 110  #> Test loss: -2.026279  #> = Starting epoch 111  #> Test loss: -2.020371  #> = Starting epoch 112  #> Test loss: -2.027358  #> = Starting epoch 113  #> Test loss: -2.004359  #> = Starting epoch 114  #> Test loss: -2.000253  #> = Starting epoch 115  #> Test loss: -2.011775  #> = Starting epoch 116  #> Test loss: -2.023753  #> = Starting epoch 117  #> Test loss: -2.018657  #> = Starting epoch 118  #> Test loss: -2.017733  #> = Starting epoch 119  #> Test loss: -1.993375  #> = Starting epoch 120  #> Test loss: -2.021626  #> = Starting epoch 121  #> Test loss: -2.044446  #> = Starting epoch 122  #> Test loss: -2.043306  #> = Starting epoch 123  #> Test loss: -1.983582  #> = Starting epoch 124  #> Test loss: -2.016671  #> = Starting epoch 125  #> Test loss: -1.99425  #> = Starting epoch 126  #> Test loss: -1.989632  #> = Starting epoch 127  #> Test loss: -2.016616  #> = Starting epoch 128  #> Test loss: -2.018335 # Generate 1024 samples for each of the first 4 conditioning variables in the test set test_samples <- as_array(generate_from_conditional_flow(conditioning_flow, 1024, test_set$conditioning[1 : 4, ]))  par(mfrow = c(4, 2)) for (i in 1 : 4) {   hist(test_samples[, i, 1], main = '', xlab = 'mu', freq = FALSE, breaks = 32, xlim = c(-3, 3))   abline(v = as_array(test_set$conditioning[i, ]), col = 'blue')   abline(v = as_array(test_set$target[i, 1]), col = 'red')   hist(test_samples[, i, 2], main = '', xlab = 'sigma', freq = FALSE, breaks = 32, xlim = c(0, 3))   abline(v = as_array(test_set$target[i, 2]), col = 'red') } par(mfrow = c(2, 2)) for (i in 1 : 4) {   plot(test_samples[, i, 1], test_samples[, i, 2], main = '', xlab = 'mu', ylab = 'sigma', xlim = c(-3, 3), ylim = c(0, 3))   abline(v = as_array(test_set$target[i, 1]), col = 'red')   abline(h = as_array(test_set$target[i, 2]), col = 'red') }"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"using-a-summarizing-network","dir":"Articles","previous_headings":"","what":"Using a summarizing network","title":"torchflow: Normalizing Flows using R torch","text":"example, conditioning variable y contains four replicates conditioning variable. ignored fact replicates, trained flow though dependent. can instead use summarizing network individually processes individual replicate set summary statistics, combine summary statistics permutation invariant way form conditioning variable. example summarizing network: can combine summarizing network flow nn_summarizing_conditional_flow object: Let’s also expand number replicated observations 32: Let’s train model: can generate samples trained model :  can also plot samples scatter plot:","code":"summarizing_network <- nn_module(   initialize = function() {     self$summary_head <- nn_sequential(       nn_linear(1, 32),       nn_relu(),       nn_linear(32, 32),       nn_relu(),       nn_linear(32, 8),     )   },   forward = function(y) {     summaries <- self$summary_head(torch_unsqueeze(y, -1))     torch_sum(summaries, -2)   } )  summary_model <- summarizing_network() summary_model(torch_randn(5, 4)) #> torch_tensor #> -0.7071 -0.4450  0.8973  0.8394  0.6094 -1.7677 -0.4467 -1.6985 #> -1.2254 -0.4747  0.8935  1.4770 -0.1325 -1.8353 -0.9247 -2.7288 #> -0.9723 -0.5416  0.8720  1.2070  0.1392 -1.8230 -0.8471 -2.1256 #> -0.7188 -0.4634  0.8998  0.8513  0.5863 -1.7680 -0.4688 -1.6968 #> -1.2502 -0.4745  0.8584  1.4739 -0.1680 -1.8842 -0.9474 -2.7824 #> [ CPUFloatType{5,8} ][ grad_fn = <SumBackward1> ] flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 8),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 8),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 8) ) conditioning_flow <- nn_summarizing_conditional_flow(summary_model, flow_model) generate_conditional_samples <- function(...) {   n_samples <- 1024   sigma <- torch_abs(torch_randn(n_samples))   mu <- torch_randn(n_samples) * sigma   y <- torch_unsqueeze(mu, 2) + torch_randn(n_samples, 32) * torch_unsqueeze(sigma, 2)   list(     target = torch_stack(list(mu, sigma), 2),     conditioning = y   ) } test_set <- generate_conditional_samples() str(test_set) #> List of 2 #>  $ target      :Float [1:1024, 1:2] #>  $ conditioning:Float [1:1024, 1:32] train_conditional_flow(   conditioning_flow,   generate_conditional_samples,   n_epochs = 128,   batch_size = 256,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(conditioning_flow(test_set$target, test_set$conditioning)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 2.527285  #> = Starting epoch 2  #> Test loss: 0.272491  #> = Starting epoch 3  #> Test loss: -1.10597  #> = Starting epoch 4  #> Test loss: -1.606657  #> = Starting epoch 5  #> Test loss: -1.878848  #> = Starting epoch 6  #> Test loss: -2.186857  #> = Starting epoch 7  #> Test loss: -2.120122  #> = Starting epoch 8  #> Test loss: -2.487521  #> = Starting epoch 9  #> Test loss: -2.417301  #> = Starting epoch 10  #> Test loss: -2.713473  #> = Starting epoch 11  #> Test loss: -2.716826  #> = Starting epoch 12  #> Test loss: -2.929903  #> = Starting epoch 13  #> Test loss: -2.868973  #> = Starting epoch 14  #> Test loss: -3.040671  #> = Starting epoch 15  #> Test loss: -3.081934  #> = Starting epoch 16  #> Test loss: -3.137375  #> = Starting epoch 17  #> Test loss: -3.179258  #> = Starting epoch 18  #> Test loss: -3.301177  #> = Starting epoch 19  #> Test loss: -3.340478  #> = Starting epoch 20  #> Test loss: -3.401419  #> = Starting epoch 21  #> Test loss: -3.246785  #> = Starting epoch 22  #> Test loss: -3.316793  #> = Starting epoch 23  #> Test loss: -3.418037  #> = Starting epoch 24  #> Test loss: -3.470295  #> = Starting epoch 25  #> Test loss: -3.461064  #> = Starting epoch 26  #> Test loss: -3.50052  #> = Starting epoch 27  #> Test loss: -3.466089  #> = Starting epoch 28  #> Test loss: -3.484479  #> = Starting epoch 29  #> Test loss: -3.478055  #> = Starting epoch 30  #> Test loss: -3.436536  #> = Starting epoch 31  #> Test loss: -3.312264  #> = Starting epoch 32  #> Test loss: -3.392005  #> = Starting epoch 33  #> Test loss: -3.554828  #> = Starting epoch 34  #> Test loss: -3.498564  #> = Starting epoch 35  #> Test loss: -3.470842  #> = Starting epoch 36  #> Test loss: -3.570588  #> = Starting epoch 37  #> Test loss: -3.553841  #> = Starting epoch 38  #> Test loss: -3.50714  #> = Starting epoch 39  #> Test loss: -3.521438  #> = Starting epoch 40  #> Test loss: -3.502527  #> = Starting epoch 41  #> Test loss: -3.485194  #> = Starting epoch 42  #> Test loss: -3.458437  #> = Starting epoch 43  #> Test loss: -3.500523  #> = Starting epoch 44  #> Test loss: -3.55771  #> = Starting epoch 45  #> Test loss: -3.622029  #> = Starting epoch 46  #> Test loss: -3.655207  #> = Starting epoch 47  #> Test loss: -3.658111  #> = Starting epoch 48  #> Test loss: -3.701914  #> = Starting epoch 49  #> Test loss: -3.742097  #> = Starting epoch 50  #> Test loss: -3.674263  #> = Starting epoch 51  #> Test loss: -3.650943  #> = Starting epoch 52  #> Test loss: -3.664033  #> = Starting epoch 53  #> Test loss: -3.608829  #> = Starting epoch 54  #> Test loss: -3.687944  #> = Starting epoch 55  #> Test loss: -3.736233  #> = Starting epoch 56  #> Test loss: -3.664591  #> = Starting epoch 57  #> Test loss: -3.735357  #> = Starting epoch 58  #> Test loss: -3.722732  #> = Starting epoch 59  #> Test loss: -3.747531  #> = Starting epoch 60  #> Test loss: -3.730324  #> = Starting epoch 61  #> Test loss: -3.635703  #> = Starting epoch 62  #> Test loss: -3.712051  #> = Starting epoch 63  #> Test loss: -3.73139  #> = Starting epoch 64  #> Test loss: -3.635473  #> = Starting epoch 65  #> Test loss: -3.765982  #> = Starting epoch 66  #> Test loss: -3.720483  #> = Starting epoch 67  #> Test loss: -3.77317  #> = Starting epoch 68  #> Test loss: -3.612534  #> = Starting epoch 69  #> Test loss: -3.78901  #> = Starting epoch 70  #> Test loss: -3.550354  #> = Starting epoch 71  #> Test loss: -3.592261  #> = Starting epoch 72  #> Test loss: -3.755776  #> = Starting epoch 73  #> Test loss: -3.484489  #> = Starting epoch 74  #> Test loss: -3.711726  #> = Starting epoch 75  #> Test loss: -3.778238  #> = Starting epoch 76  #> Test loss: -3.756815  #> = Starting epoch 77  #> Test loss: -3.773376  #> = Starting epoch 78  #> Test loss: -3.740174  #> = Starting epoch 79  #> Test loss: -3.783231  #> = Starting epoch 80  #> Test loss: -3.439708  #> = Starting epoch 81  #> Test loss: -3.623181  #> = Starting epoch 82  #> Test loss: -3.578712  #> = Starting epoch 83  #> Test loss: -3.76767  #> = Starting epoch 84  #> Test loss: -3.692988  #> = Starting epoch 85  #> Test loss: -3.743431  #> = Starting epoch 86  #> Test loss: -3.669569  #> = Starting epoch 87  #> Test loss: -3.70144  #> = Starting epoch 88  #> Test loss: -3.713464  #> = Starting epoch 89  #> Test loss: -3.753165  #> = Starting epoch 90  #> Test loss: -3.746385  #> = Starting epoch 91  #> Test loss: -3.7517  #> = Starting epoch 92  #> Test loss: -3.480921  #> = Starting epoch 93  #> Test loss: -3.635423  #> = Starting epoch 94  #> Test loss: -3.72131  #> = Starting epoch 95  #> Test loss: -3.737788  #> = Starting epoch 96  #> Test loss: -3.784837  #> = Starting epoch 97  #> Test loss: -3.795456  #> = Starting epoch 98  #> Test loss: -3.781871  #> = Starting epoch 99  #> Test loss: -3.652484  #> = Starting epoch 100  #> Test loss: -3.669938  #> = Starting epoch 101  #> Test loss: -3.80552  #> = Starting epoch 102  #> Test loss: -3.783965  #> = Starting epoch 103  #> Test loss: -3.703072  #> = Starting epoch 104  #> Test loss: -3.759994  #> = Starting epoch 105  #> Test loss: -3.812117  #> = Starting epoch 106  #> Test loss: -3.797255  #> = Starting epoch 107  #> Test loss: -3.802492  #> = Starting epoch 108  #> Test loss: -3.76484  #> = Starting epoch 109  #> Test loss: -3.829921  #> = Starting epoch 110  #> Test loss: -3.840332  #> = Starting epoch 111  #> Test loss: -3.816914  #> = Starting epoch 112  #> Test loss: -3.792278  #> = Starting epoch 113  #> Test loss: -3.722486  #> = Starting epoch 114  #> Test loss: -3.774091  #> = Starting epoch 115  #> Test loss: -3.814323  #> = Starting epoch 116  #> Test loss: -3.787137  #> = Starting epoch 117  #> Test loss: -3.784008  #> = Starting epoch 118  #> Test loss: -3.793586  #> = Starting epoch 119  #> Test loss: -3.830618  #> = Starting epoch 120  #> Test loss: -3.723323  #> = Starting epoch 121  #> Test loss: -3.819359  #> = Starting epoch 122  #> Test loss: -3.674448  #> = Starting epoch 123  #> Test loss: -3.740015  #> = Starting epoch 124  #> Test loss: -3.755173  #> = Starting epoch 125  #> Test loss: -3.842011  #> = Starting epoch 126  #> Test loss: -3.859711  #> = Starting epoch 127  #> Test loss: -3.846105  #> = Starting epoch 128  #> Test loss: -3.729074 # Generate 1024 samples for each of the first 4 conditioning variables in the test set test_samples <- as_array(generate_from_conditional_flow(conditioning_flow, 1024, test_set$conditioning[1 : 4, ]))  par(mfrow = c(4, 2)) for (i in 1 : 4) {   hist(test_samples[, i, 1], main = '', xlab = 'mu', freq = FALSE, breaks = 32, xlim = c(-3, 3))   abline(v = as_array(test_set$target[i, 1]), col = 'red')   hist(test_samples[, i, 2], main = '', xlab = 'sigma', freq = FALSE, breaks = 32, xlim = c(0, 3))   abline(v = as_array(test_set$target[i, 2]), col = 'red') } par(mfrow = c(2, 2)) for (i in 1 : 4) {   plot(test_samples[, i, 1], test_samples[, i, 2], main = '', xlab = 'mu', ylab = 'sigma', xlim = c(-3, 3), ylim = c(0, 3))   abline(v = as_array(test_set$target[i, 1]), col = 'red')   abline(h = as_array(test_set$target[i, 2]), col = 'red') }"},{"path":"https://mbertolacci.github.io/torchflow/articles/torchflow.html","id":"more-complex-conditioning-variables","dir":"Articles","previous_headings":"","what":"More complex conditioning variables","title":"torchflow: Normalizing Flows using R torch","text":"summarizing network key using complex conditioning variables. example, can use 2D grid points conditioning variable, processed summarizing network set summary statistics used conditioning variable flow. 2-D grid, convolutional network conventional choice often works well practice. following example generates data using exponential covariance unknown variance length scale 16x16 2-D grid:  can now create summarizing network conditioning variable. convolutional network conventional choice type data. network alternates convolution, ReLU max pooling layers, final adaptive average pooling layer reduce summary statistics fixed size vector dimension 32 (number summary statistics used flow): can now create conditional flow summarizing network: can now train model : can now generate samples trained model:  can also plot samples scatter plot:","code":"n_grid <- 16 x_y_grid <- as.matrix(expand.grid(   x = seq(0, 1, length.out = n_grid),   y = seq(0, 1, length.out = n_grid) )) distances <- torch_tensor(as.matrix(dist(x_y_grid)))  generate_conditional_samples <- function(...) {   n_samples <- 1024   ell <- 0.1 + 1.9 * torch_rand(n_samples)   sigma <- torch_abs(torch_randn(n_samples))    Sigma <- torch_square(sigma)$unsqueeze(-1)$unsqueeze(-1) * torch_exp(-torch_unsqueeze(distances, 1) / ell$unsqueeze(-1)$unsqueeze(-1))   L <- linalg_cholesky(Sigma)   y_flat <- torch_matmul(L, torch_randn(n_samples, 256, 1))   # The extra 1 here is interpreted as a single channel   y <- torch_reshape(y_flat, c(n_samples, 1, n_grid, n_grid))    list(     # We log the parameters to ensure they have real support;     # this is not strictly necessary for the flow to work, but it     # does make it easier to match the distribution     target = torch_log(torch_stack(list(ell, sigma), 2)),     conditioning = y   ) }  test_set <- generate_conditional_samples() par(mfrow = c(2, 2)) for (i in 1 : 4) {   image(as_array(test_set$conditioning[i, 1, , ]), main = '', xlab = 'x', ylab = 'y') } summary_model <- nn_sequential(   nn_conv2d(1, 16, 3, padding = 1),   nn_relu(),   nn_max_pool2d(2),   nn_conv2d(16, 32, 3, padding = 1),   nn_relu(),   nn_adaptive_avg_pool2d(1),   nn_flatten() )  str(summary_model(test_set$conditioning[1 : 10, , , drop = FALSE])) #> Float [1:10, 1:32] flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 32),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 32),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 32) ) conditioning_flow <- nn_summarizing_conditional_flow(summary_model, flow_model) train_conditional_flow(   conditioning_flow,   generate_conditional_samples,   n_epochs = 128,   batch_size = 1024,   after_epoch = function(...) {     test_loss <- as_array(forward_kl_loss(conditioning_flow(test_set$target, test_set$conditioning)))     cat('Test loss:', test_loss, '\\n')   } ) #> = Starting epoch 1  #> Test loss: 0.9148552  #> = Starting epoch 2  #> Test loss: 0.8257918  #> = Starting epoch 3  #> Test loss: 0.76542  #> = Starting epoch 4  #> Test loss: 0.7236527  #> = Starting epoch 5  #> Test loss: 0.6900516  #> = Starting epoch 6  #> Test loss: 0.6569164  #> = Starting epoch 7  #> Test loss: 0.6210347  #> = Starting epoch 8  #> Test loss: 0.5757983  #> = Starting epoch 9  #> Test loss: 0.5162966  #> = Starting epoch 10  #> Test loss: 0.4483618  #> = Starting epoch 11  #> Test loss: 0.3765357  #> = Starting epoch 12  #> Test loss: 0.3034223  #> = Starting epoch 13  #> Test loss: 0.2256234  #> = Starting epoch 14  #> Test loss: 0.1406085  #> = Starting epoch 15  #> Test loss: 0.0433929  #> = Starting epoch 16  #> Test loss: -0.06104606  #> = Starting epoch 17  #> Test loss: -0.1640665  #> = Starting epoch 18  #> Test loss: -0.2555541  #> = Starting epoch 19  #> Test loss: -0.3497666  #> = Starting epoch 20  #> Test loss: -0.4393754  #> = Starting epoch 21  #> Test loss: -0.5053609  #> = Starting epoch 22  #> Test loss: -0.4949403  #> = Starting epoch 23  #> Test loss: -0.612844  #> = Starting epoch 24  #> Test loss: -0.5932434  #> = Starting epoch 25  #> Test loss: -0.6804665  #> = Starting epoch 26  #> Test loss: -0.4698762  #> = Starting epoch 27  #> Test loss: -0.7435459  #> = Starting epoch 28  #> Test loss: -0.7038008  #> = Starting epoch 29  #> Test loss: -0.8489974  #> = Starting epoch 30  #> Test loss: -0.769849  #> = Starting epoch 31  #> Test loss: -0.7708951  #> = Starting epoch 32  #> Test loss: -0.9687425  #> = Starting epoch 33  #> Test loss: -0.9050489  #> = Starting epoch 34  #> Test loss: -1.019698  #> = Starting epoch 35  #> Test loss: -1.045637  #> = Starting epoch 36  #> Test loss: -0.9476085  #> = Starting epoch 37  #> Test loss: -1.161181  #> = Starting epoch 38  #> Test loss: -1.138703  #> = Starting epoch 39  #> Test loss: -1.20327  #> = Starting epoch 40  #> Test loss: -1.252636  #> = Starting epoch 41  #> Test loss: -1.238665  #> = Starting epoch 42  #> Test loss: -1.361016  #> = Starting epoch 43  #> Test loss: -1.345004  #> = Starting epoch 44  #> Test loss: -1.4338  #> = Starting epoch 45  #> Test loss: -1.405056  #> = Starting epoch 46  #> Test loss: -1.461778  #> = Starting epoch 47  #> Test loss: -1.521416  #> = Starting epoch 48  #> Test loss: -1.504567  #> = Starting epoch 49  #> Test loss: -1.605985  #> = Starting epoch 50  #> Test loss: -1.579659  #> = Starting epoch 51  #> Test loss: -1.658729  #> = Starting epoch 52  #> Test loss: -1.64879  #> = Starting epoch 53  #> Test loss: -1.712619  #> = Starting epoch 54  #> Test loss: -1.747455  #> = Starting epoch 55  #> Test loss: -1.770522  #> = Starting epoch 56  #> Test loss: -1.794251  #> = Starting epoch 57  #> Test loss: -1.801921  #> = Starting epoch 58  #> Test loss: -1.74546  #> = Starting epoch 59  #> Test loss: -1.647608  #> = Starting epoch 60  #> Test loss: -1.857547  #> = Starting epoch 61  #> Test loss: -1.780075  #> = Starting epoch 62  #> Test loss: -1.615994  #> = Starting epoch 63  #> Test loss: -1.903093  #> = Starting epoch 64  #> Test loss: -1.679343  #> = Starting epoch 65  #> Test loss: -1.79478  #> = Starting epoch 66  #> Test loss: -1.818268  #> = Starting epoch 67  #> Test loss: -1.737698  #> = Starting epoch 68  #> Test loss: -1.936016  #> = Starting epoch 69  #> Test loss: -1.83212  #> = Starting epoch 70  #> Test loss: -1.961082  #> = Starting epoch 71  #> Test loss: -1.819783  #> = Starting epoch 72  #> Test loss: -1.972634  #> = Starting epoch 73  #> Test loss: -1.924888  #> = Starting epoch 74  #> Test loss: -1.979099  #> = Starting epoch 75  #> Test loss: -1.969754  #> = Starting epoch 76  #> Test loss: -1.937117  #> = Starting epoch 77  #> Test loss: -2.049493  #> = Starting epoch 78  #> Test loss: -1.893825  #> = Starting epoch 79  #> Test loss: -2.032295  #> = Starting epoch 80  #> Test loss: -2.045876  #> = Starting epoch 81  #> Test loss: -1.976756  #> = Starting epoch 82  #> Test loss: -2.092072  #> = Starting epoch 83  #> Test loss: -1.947967  #> = Starting epoch 84  #> Test loss: -2.081878  #> = Starting epoch 85  #> Test loss: -2.046363  #> = Starting epoch 86  #> Test loss: -2.057215  #> = Starting epoch 87  #> Test loss: -2.13402  #> = Starting epoch 88  #> Test loss: -2.018486  #> = Starting epoch 89  #> Test loss: -2.082926  #> = Starting epoch 90  #> Test loss: -2.118807  #> = Starting epoch 91  #> Test loss: -1.909981  #> = Starting epoch 92  #> Test loss: -2.007208  #> = Starting epoch 93  #> Test loss: -2.111677  #> = Starting epoch 94  #> Test loss: -1.874924  #> = Starting epoch 95  #> Test loss: -2.113178  #> = Starting epoch 96  #> Test loss: -2.046612  #> = Starting epoch 97  #> Test loss: -2.03427  #> = Starting epoch 98  #> Test loss: -2.13606  #> = Starting epoch 99  #> Test loss: -2.064483  #> = Starting epoch 100  #> Test loss: -2.160323  #> = Starting epoch 101  #> Test loss: -2.06707  #> = Starting epoch 102  #> Test loss: -2.165398  #> = Starting epoch 103  #> Test loss: -2.072236  #> = Starting epoch 104  #> Test loss: -2.178817  #> = Starting epoch 105  #> Test loss: -2.085823  #> = Starting epoch 106  #> Test loss: -2.178226  #> = Starting epoch 107  #> Test loss: -2.19351  #> = Starting epoch 108  #> Test loss: -2.082016  #> = Starting epoch 109  #> Test loss: -2.197536  #> = Starting epoch 110  #> Test loss: -2.201848  #> = Starting epoch 111  #> Test loss: -2.188328  #> = Starting epoch 112  #> Test loss: -2.258529  #> = Starting epoch 113  #> Test loss: -2.236154  #> = Starting epoch 114  #> Test loss: -2.240849  #> = Starting epoch 115  #> Test loss: -2.257506  #> = Starting epoch 116  #> Test loss: -2.236174  #> = Starting epoch 117  #> Test loss: -2.238575  #> = Starting epoch 118  #> Test loss: -2.280871  #> = Starting epoch 119  #> Test loss: -2.267398  #> = Starting epoch 120  #> Test loss: -2.269904  #> = Starting epoch 121  #> Test loss: -2.291243  #> = Starting epoch 122  #> Test loss: -2.252173  #> = Starting epoch 123  #> Test loss: -2.285157  #> = Starting epoch 124  #> Test loss: -2.302592  #> = Starting epoch 125  #> Test loss: -2.298436  #> = Starting epoch 126  #> Test loss: -2.258844  #> = Starting epoch 127  #> Test loss: -2.261136  #> = Starting epoch 128  #> Test loss: -2.240029 test_samples <- as_array(generate_from_conditional_flow(conditioning_flow, 1024, test_set$conditioning[1 : 4, , , drop = FALSE])) str(test_samples) #>  num [1:1024, 1:4, 1:2] 0.119 -0.153 -1.035 0.141 0.444 ...  test_target <- as_array(test_set$target)  par(mfrow = c(4, 2)) for (i in 1 : 4) {   hist(exp(test_samples[, i, 1]), main = '', xlab = 'ell', freq = FALSE, breaks = 32, xlim = c(0, 2))   abline(v = exp(test_target[i, 1]), col = 'red')   hist(exp(test_samples[, i, 2]), main = '', xlab = 'sigma', freq = FALSE, breaks = 32, xlim = c(0, 3))   abline(v = exp(test_target[i, 2]), col = 'red') } par(mfrow = c(2, 2)) for (i in 1 : 4) {   plot(exp(test_samples[, i, 1]), exp(test_samples[, i, 2]), main = '', xlab = 'ell', ylab = 'sigma', xlim = c(0, 2), ylim = c(0, 3))   abline(v = exp(test_target[i, 1]), col = 'red')   abline(h = exp(test_target[i, 2]), col = 'red') }"},{"path":"https://mbertolacci.github.io/torchflow/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael Bertolacci. Author, maintainer.","code":""},{"path":"https://mbertolacci.github.io/torchflow/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bertolacci M (2024). torchflow: Conditional Normalizing Flows. R package version 0.0.1, https://mbertolacci.github.io/torchflow/.","code":"@Manual{,   title = {torchflow: Conditional Normalizing Flows},   author = {Michael Bertolacci},   year = {2024},   note = {R package version 0.0.1},   url = {https://mbertolacci.github.io/torchflow/}, }"},{"path":"https://mbertolacci.github.io/torchflow/index.html","id":"torchflow","dir":"","previous_headings":"","what":"Conditional Normalizing Flows","title":"Conditional Normalizing Flows","text":"Normalizing flows R using torch package. Go read vignette package website: torchflow. Fill details .","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward KL Loss — forward_kl_loss","title":"Forward KL Loss — forward_kl_loss","text":"Compute forward KL loss flow model.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward KL Loss — forward_kl_loss","text":"","code":"forward_kl_loss(input)"},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward KL Loss — forward_kl_loss","text":"input output flow model, tensor dimensions [batch1, ..., batchN, d] batch1, ..., batchN dimensions batch d dimension input. must also attribute log_jacobian containing log determinant Jacobian transformation.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/forward_kl_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forward KL Loss — forward_kl_loss","text":"","code":"library(torch) flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 0),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 0) ) x <- torch_randn(10, 2) y <- flow_model(x) loss <- forward_kl_loss(y)"},{"path":"https://mbertolacci.github.io/torchflow/reference/generate_from_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate samples from a conditional flow model — generate_from_conditional_flow","title":"Generate samples from a conditional flow model — generate_from_conditional_flow","text":"function generates samples conditional flow model. conditioning provided, n_samples_per_batch samples generated batch conditioning variable. conditioning provided, n_samples_per_batch samples generated.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/generate_from_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate samples from a conditional flow model — generate_from_conditional_flow","text":"","code":"generate_from_conditional_flow(model, n_samples_per_batch, conditioning)"},{"path":"https://mbertolacci.github.io/torchflow/reference/generate_from_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate samples from a conditional flow model — generate_from_conditional_flow","text":"model conditional flow model. n_samples_per_batch number samples generate batch conditioning variable, total number samples conditioning provided. conditioning conditioning variable, torch tensor dimensions [batch, ...] batch dimension batch ... dimensions conditioning variable.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":null,"dir":"Reference","previous_headings":"","what":"Affine Coupling Block — nn_affine_coupling_block","title":"Affine Coupling Block — nn_affine_coupling_block","text":"affine coupling block conditional flow inheriting nn_conditional_flow() applies following transformation input.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Affine Coupling Block — nn_affine_coupling_block","text":"","code":"nn_affine_coupling_block(   input_size,   conditioning_size = 0,   left_size = as.integer(input_size%/%2),   f_scale,   f_shift,   g_scale,   g_shift,   soft_clamp = 1.9 )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Affine Coupling Block — nn_affine_coupling_block","text":"input_size dimension input. input tensor dimensions [batch_size, input_size], just [input_size] batch dimension. conditioning_size dimension conditioning input, batch dimensions input. left_size dimension left part input (split \\(x_1\\) equations ). f_scale function \\(f_\\text{scale}\\) equations . , following parameters, default conditional multi-layer perceptron (MLP); see nn_conditional_mlp(). must inherit nn_conditional(). f_shift function \\(f_\\text{shift}\\) equations ; see . g_scale function \\(g_\\text{scale}\\) equations ; see . g_shift function \\(g_\\text{shift}\\) equations ; see . soft_clamp soft clamp value scale parameters.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Affine Coupling Block — nn_affine_coupling_block","text":"Let \\(x = (x_1, x_2)\\) split input two parts, let \\(u\\) conditioning input. forward transformation given : $$   y_1 = x_1 \\exp(f_\\text{scale}(x_2, u)) + f_\\text{shift}(x_2, u)   y_2 = x_2 \\exp(g_\\text{scale}(y_1, u)) + g_\\text{shift}(y_1, u) $$ inverse transformation given : $$   x_1 = y_1 \\exp(g_\\text{scale}(y_2, u)) + g_\\text{shift}(y_2, u)   x_2 = y_2 \\exp(f_\\text{scale}(x_1, u)) + f_\\text{shift}(x_1, u) $$ log determinant Jacobian transformation given : $$   \\log | \\det \\frac{\\partial y}{\\partial x} |    = \\sum_{=1}^2 f_\\text{scale}(x_i, u) + g_\\text{scale}(y_i, u) $$ performing multiple transformations sequence, can construct complex normalizing flow capable modeling complicated conditional distributions. pair transformations, dimensions input permuted using nn_permutation_flow().","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_affine_coupling_block.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Affine Coupling Block — nn_affine_coupling_block","text":"","code":"library(torch) # Coupling block used on its own with no conditioning flow_model <- nn_affine_coupling_block(2, 0) x <- torch_randn(10, 2) y <- flow_model(x) # y will be a tensor of dimensions [10, 2] x_recovered <- flow_model$reverse(y) # x_recovered will be a tensor of dimensions [10, 2] # and numerically close to the original x  # Coupling block used with conditioning flow_model <- nn_affine_coupling_block(2, 4) x <- torch_randn(10, 2) u <- torch_randn(10, 4) y <- flow_model(x, u) # y will be a tensor of dimensions [10, 2] x_recovered <- flow_model$reverse(y, u) # x_recovered will be a tensor of dimensions [10, 2] # and numerically close to the original x  # Coupling block used as part of a more complex flow model flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(2, 4),   nn_permutation_flow(2),   nn_affine_coupling_block(2, 4) ) y <- flow_model(x, u)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Module — nn_conditional","title":"Conditional Module — nn_conditional","text":"conditional module module takes additional conditioning input forward pass.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Module — nn_conditional","text":"","code":"nn_conditional()"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional.html","id":"forward-method","dir":"Reference","previous_headings":"","what":"Forward method","title":"Conditional Module — nn_conditional","text":"forward method take two arguments, input conditioning, return output. Example:","code":"forward = function(input, conditioning) {   output <- ...   output }"},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Normalizing Flow — nn_conditional_flow","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"conditional normalizing flow normalizing flow takes additional conditioning input. module provides base class conditional normalizing flows.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"","code":"nn_conditional_flow()"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"base class, nn_conditional_flow, abstract class provides forward reverse method, well dimension method. Subclasses created torch::nn_module() implement methods. class subclass torch::nn_module(), inherits methods semantics.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"forward-method","dir":"Reference","previous_headings":"","what":"Forward method","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"forward method return output log determinant Jacobian attribute log_jacobian. Example:","code":"forward = function(input, conditioning) {   output <- ...   attr(output, 'log_jacobian') <- ...   output }"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"reverse-method","dir":"Reference","previous_headings":"","what":"Reverse method","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"reverse method return inverse output, need implement log determinant. Example:","code":"reverse = function(input, conditioning) {   output <- ...   output }"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_flow.html","id":"dimension-method","dir":"Reference","previous_headings":"","what":"Dimension method","title":"Conditional Normalizing Flow — nn_conditional_flow","text":"dimension method return dimension input output flow. Example:","code":"dimension = function() {   return(2) }"},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Multilayer Perceptron — nn_conditional_mlp","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"conditional multilayer perceptron multilayer perceptron takes additional conditioning input. inherits nn_conditional(); normalizing flow. practice, regular input conditioning input concatenated passed MLP.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"","code":"nn_conditional_mlp(   input_size,   conditioning_size,   output_size,   layer_sizes = c(128, 128),   activation = nn_relu )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"input_size size input MLP. conditioning_size size conditioning input MLP. output_size size output MLP. layer_sizes vector integers specifying number neurons layer. can NULL, case single linear layer used. activation activation function use layer.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_conditional_mlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Multilayer Perceptron — nn_conditional_mlp","text":"","code":"library(torch) mlp <- nn_conditional_mlp(10, 5, 1) input <- torch_randn(10) conditioning <- torch_randn(5) output <- mlp(input, conditioning)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation Flow — nn_permutation_flow","title":"Permutation Flow — nn_permutation_flow","text":"permutation flow conditional flow inheriting nn_conditional_flow() permutes input dimensions. permutation fixed random permutation initialization change. log Jacobian zero since simple reordering input dimensions.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Flow — nn_permutation_flow","text":"","code":"nn_permutation_flow(input_size)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Flow — nn_permutation_flow","text":"input_size size input flow.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_permutation_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Permutation Flow — nn_permutation_flow","text":"","code":"library(torch) # Use on its own permutation_flow <- nn_permutation_flow(10) input <- torch_randn(10) output <- permutation_flow(input) # Use in a more complex conditional flow flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(10, 5),   nn_permutation_flow(10),   nn_affine_coupling_block(10, 5) )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Sequential Conditional Flow — nn_sequential_conditional_flow","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"sequential conditional flow conditional flow inheriting nn_conditional_flow() applies sequence conditional flows, passing conditioning input flow. analog torch::nn_sequential() conditional flows.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"","code":"nn_sequential_conditional_flow(...)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"... sequence conditional flows, list conditional flows.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_sequential_conditional_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sequential Conditional Flow — nn_sequential_conditional_flow","text":"","code":"flow_model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(10, 5),   nn_permutation_flow(10),   nn_affine_coupling_block(10, 5) )"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"summarizing conditional flow conditional flow summarizes conditioning input using summary model. inherits nn_conditional_flow(), requires summary_model flow_model initializer. forward pass, summary model applied conditioning input, result passed flow model. can used reduce dimensionality conditioning input.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"","code":"nn_summarizing_conditional_flow(summary_model, flow_model)"},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"summary_model torch::nn_module() summarizes conditioning input. flow_model torch::nn_module() conditional flow.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"forward() method optional summary argument. summary provided, used conditioning input flow model, skipping application summary model. can used speed forward pass conditioning input used multiple times. class also provides summarize() method can used compute summary conditioning input outside forward pass.","code":""},{"path":[]},{"path":"https://mbertolacci.github.io/torchflow/reference/nn_summarizing_conditional_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizing Conditional Flow — nn_summarizing_conditional_flow","text":"","code":"library(torch) summary_model <- nn_sequential(   nn_linear(10, 5),   nn_relu() ) flow_model <- nn_affine_coupling_block(10, 5) summarizing_flow <- nn_summarizing_conditional_flow(summary_model, flow_model)"},{"path":"https://mbertolacci.github.io/torchflow/reference/torchflow-package.html","id":null,"dir":"Reference","previous_headings":"","what":"torchflow: Conditional Normalizing Flows — torchflow-package","title":"torchflow: Conditional Normalizing Flows — torchflow-package","text":"package fit conditional normalizing flows.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/torchflow-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"torchflow: Conditional Normalizing Flows — torchflow-package","text":"Maintainer: Michael Bertolacci m.bertolacci@gmail.com (ORCID)","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":null,"dir":"Reference","previous_headings":"","what":"Train a conditional flow model — train_conditional_flow","title":"Train a conditional flow model — train_conditional_flow","text":"Method train conditional flow model. basic training loop following steps:","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train a conditional flow model — train_conditional_flow","text":"","code":"train_conditional_flow(   model,   generate,   optimizer = torch::optim_adam,   n_epochs = 128,   batch_size = 32,   after_epoch = NULL,   verbose = TRUE,   ... )"},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train a conditional flow model — train_conditional_flow","text":"model conditional flow model inheriting nn_conditional_flow(). generate function generates batch target conditioning samples; see details. passed current epoch number argument. optimizer optimizer use, e.g. torch::optim_adam(). n_epochs number epochs train . batch_size batch size. after_epoch function call epoch. verbose Whether print progress. ... Additional arguments pass optimizer.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Train a conditional flow model — train_conditional_flow","text":"training algorithm follows. epoch: Generate (using generate) batch target conditioning samples. Loop batches epoch, performing gradient descent step batch. batches processed order generated samples. Call after_epoch (provided) current epoch generated samples. can used print test loss tasks. generate function (called current epoch number argument) return list following elements: target: array(), matrix(), torch::torch_tensor() target samples. conditioning: optional array(), matrix(), torch::torch_tensor() conditioning samples. returning torch_tensor() objects, take care device model. generated samples choice user. generate new samples epoch, share samples across epochs (noting model may overfit case). latter case, good permute order samples epoch. training may stopped early. original model object modified place.","code":""},{"path":"https://mbertolacci.github.io/torchflow/reference/train_conditional_flow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train a conditional flow model — train_conditional_flow","text":"","code":"library(torch) model <- nn_sequential_conditional_flow(   nn_affine_coupling_block(input_size = 2),   nn_permutation_flow(input_size = 2),   nn_affine_coupling_block(input_size = 2) ) generate <- function(epoch) {   list(target = 2 + torch_randn(1024, 2)) } # In practice, the number of epochs should be larger train_conditional_flow(model, generate, n_epochs = 2) #> = Starting epoch 1  #> = Starting epoch 2"}]
